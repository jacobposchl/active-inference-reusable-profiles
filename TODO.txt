Complete Experimental Protocol
Your experiment consists of three stages that follow the generate-fit-evaluate paradigm, and I'll specify exactly what happens at each stage with all five reference generators included.
Stage One: Reference Data Generation
In the generation stage, you will create behavioral datasets from five different generative processes. Three of these are your core active inference models (M1, M2, and M3), and two are baseline policies (epsilon-greedy and softmax Q-learning) that provide additional reference points for model comparison. The inclusion of these baseline policies is methodologically important because they represent fundamentally different computational architectures. If your active inference models can distinguish themselves not only from each other but also from model-free reinforcement learning approaches, this strengthens the claim that active inference mechanisms are empirically identifiable.
For each of the five generators, you will produce twenty independent runs of eight hundred trials each. Each run uses your standard two-armed bandit reversal learning task with deterministic reversals at fixed intervals. The reversal schedule should be identical across all runs and all generators so that any performance differences reflect the computational mechanisms rather than environmental variability. I recommend using reversal intervals of forty, eighty, or one hundred sixty trials based on what you've tested previously, and keeping this consistent throughout. Each run should use a different random seed for the stochastic elements (reward outcomes, hint observations, action sampling), but the underlying reversal schedule remains fixed.
For the M1 generator, you'll run agents with static global precision and outcome preferences. The specific parameter values should be reasonable defaults from your configuration file, such as gamma equal to two point five and outcome preference logits that clearly favor reward over loss. For the M2 generator, you'll run agents with entropy-coupled dynamic precision, using default values like gamma base equal to two point five and entropy coupling constant k equal to one point zero. For the M3 generator, you'll run agents with the full profile-based architecture using your two-profile setup with context-appropriate assignments in the Z matrix.
For the epsilon-greedy baseline, you implement a simple model-free policy that maintains action values for each arm and selects the best action with probability one minus epsilon, or explores randomly with probability epsilon. A reasonable epsilon value would be zero point one, meaning ten percent random exploration. The agent updates action values using a simple delta rule with learning rate alpha around zero point one. This provides a reference for how well a basic explore-exploit heuristic can perform on your task without any model-based inference.
For the softmax baseline, you implement a similar Q-learning approach but use softmax action selection instead of epsilon-greedy. The agent maintains action values and selects actions according to probabilities proportional to the exponential of the values divided by a temperature parameter. A temperature around one point zero provides reasonable stochasticity. This baseline differs from epsilon-greedy in using graded stochasticity rather than discrete random exploration.
The critical data to save from each generated run includes the complete action sequence (what the agent chose on each trial), the complete observation sequence (what outcomes the environment returned), the true latent context on each trial (which arm was actually better), and for the active inference models, the belief trajectory over contexts and the policy precision values. This last element is important because when you later fit models to this data, you'll want to be able to compare whether the fitted model's inferred beliefs match the generator's actual beliefs.
Stage Two: Parameter Fitting with Cross-Validation
In the fitting stage, you take each of the one hundred generated runs (twenty runs times five generators) and fit all three active inference models to it using five-fold cross-validation. Note that you only fit the active inference models, not the baseline policies, because the baselines don't have the same parameter structure and aren't candidates for explaining active inference behavior. The goal is to determine which active inference model best explains each type of generated behavior.
For each of the one hundred runs, you split the eight hundred trials into five folds of one hundred sixty trials each. You then perform five rounds of fitting, where in each round you use four folds as training data and hold out one fold as test data. This gives you five train-test splits per run, and you'll average the test performance across these five splits to get a robust estimate of how well each model predicts held-out data.
The parameter fitting happens on the training folds using your parallelized grid search. For M1, you search over a grid of gamma values ranging from perhaps zero point five to sixteen, and outcome preference logit magnitudes. The specific grid points should be logarithmically spaced for gamma (like zero point five, one, two, four, eight, sixteen) because this parameter operates on a multiplicative scale. For the preference logits, you might search over magnitudes like three, five, seven, nine, keeping the structure of favoring reward over loss but varying the strength.
For M2, you search over gamma base (same range as M1's gamma), entropy coupling constant k (ranging from zero point zero five to four), and the same preference logit magnitudes. The entropy coupling constant determines how strongly precision responds to uncertainty, so you want to cover a range from weak coupling to strong coupling.
For M3, the parameter space is larger because you have profile-specific parameters. For each of the two profiles, you search over gamma values, outcome preference logit magnitudes, and policy prior logit magnitudes (the xi parameters that bias action selection). You may choose to hold the Z matrix fixed at your theoretically motivated assignment (profiles aligned with contexts) or include it in the search space if computational budget allows. Given that you're already searching over many parameters, I'd recommend starting with Z fixed and only varying it if you find that the fixed assignment is producing poor fits.
The parallelization works by creating a list of all parameter combinations to evaluate, submitting each as an independent job to your ProcessPoolExecutor, and collecting the results. Each job computes the log-likelihood of the training data under that parameter combination by running the inference forward (not re-fitting, just evaluating). The parameter combination with the highest training log-likelihood is selected as the best fit for that fold.
Once you have the best parameters for each training fold, you evaluate those parameters on the corresponding test fold to get the test log-likelihood. This test log-likelihood is what you'll use for model comparison, because it estimates how well the model predicts new data it hasn't seen during fitting. You then average the test log-likelihoods across the five folds to get a single cross-validated log-likelihood value for that model on that run.
Stage Three: Evaluation and Aggregation
In the evaluation stage, you compute your metrics for each generator-model pair and build your confusion matrices. For each of the five generators and each of the three active inference models, you have twenty runs. For each run, you have a cross-validated log-likelihood from stage two. You compute the mean and standard error of the log-likelihood across the twenty runs to get a single entry in your log-likelihood confusion matrix with uncertainty quantification.
You also compute AIC and BIC for each model-run combination. The formulas are AIC equals two times k minus two times log-likelihood, and BIC equals k times log of n minus two times log-likelihood, where k is the number of free parameters in the model (three for M1, four for M2, fourteen for M3), n is the number of observations (eight hundred trials), and the log-likelihood is the cross-validated value from stage two. You compute these for each of the twenty runs and then average them to get mean AIC and BIC values for each generator-model pair.
For accuracy, you compute what proportion of trials the fitted model selects the currently optimal action (the action that matches the true latent context). This requires running the fitted model forward to generate predictions rather than just evaluating likelihoods. For each parameter combination that was selected as best during fitting, you run that model through the full sequence of observations from the test fold and record what actions it would choose at each step. You then compare these predicted actions to the true optimal actions and compute the match rate. You average this across test folds and across runs to get mean accuracy for each generator-model pair.
Comprehensive Data Output Specification
Now let me specify exactly what data you need to save and in what format, organized by the level of granularity from most detailed to most aggregated. The reason for this hierarchical structure is that you want both the raw material for detailed diagnostic checking and the summary statistics for quick interpretation and manuscript presentation.
Trial-Level Data Files
For each combination of generator, model, and run, you should save a detailed trial-by-trial CSV file. This means you'll have three hundred CSV files in total (five generators times three models times twenty runs). Each file should be named descriptively, such as "trial_data_gen_M3_model_M1_run_005.csv", so you can easily identify which generator produced the data, which model was fitted to it, and which run it represents.
Each row in this CSV represents a single trial and should contain the following columns. First, you need trial metadata: the trial index (zero to seven hundred ninety-nine), the cross-validation fold that trial belongs to (zero to four), and whether this trial was in the training set or test set for this particular fold. Next, you need the ground truth from the generator: the true latent context on this trial (left better or right better), the action the generator actually took (act left, act right, act hint, or act start), and the observations the generator received (hint observation, reward observation, choice observation).
Then you need the fitted model's predictions and internal states. This includes the action the fitted model would have selected on this trial given the observations up to that point, the probability the fitted model assigned to each possible action (so you can see how confident it was), the fitted model's inferred beliefs over latent contexts after processing this trial's observations, and the fitted model's policy precision on this trial if applicable. You also need the single-trial log-likelihood, which is the log probability the fitted model assigned to the action that was actually taken. This is the pointwise contribution to the total log-likelihood for this trial.
Finally, you need derived metrics computed from the model's prediction and the ground truth. This includes a binary accuracy indicator (one if the fitted model's predicted action matches the currently optimal action, zero otherwise), whether this trial was a reversal point, and any other task-specific features you might want to examine later such as whether the agent requested a hint or whether the outcome was a reward or loss.
The reason for saving all this detail is that it enables post-hoc diagnostic analyses that you can't anticipate now. For example, you might later want to examine whether models show different error patterns on post-reversal trials versus stable periods. Or you might want to plot belief trajectories for specific runs where a model fit particularly well or poorly. Or you might want to compute metrics like trial-by-trial prediction entropy or information gain. Having the complete trial-level data ensures you can answer these questions without re-running the experiments.
Run-Level Data Files
For each combination of generator and model, you should save a run-level summary CSV that aggregates across the twenty runs. This file would be named something like "run_summary_gen_M3_model_M1.csv" and would have twenty rows, one for each run. Each row should contain the following information.
First, you need identifiers: the run index (zero to nineteen) and the random seed used for that run. Next, you need the fitted parameter values that were selected during cross-validation. For M1, this would be the gamma value and the outcome preference logit values. For M2, this would be gamma base, entropy coupling k, and outcome preference logits. For M3, this would be the profile-specific parameters including gammas, outcome preference logits, and policy prior logits for each profile. Saving these fitted parameters is crucial because it lets you examine what parameter values the grid search converged to, whether there's consistency across runs, and whether certain generators produce distinctive parameter signatures.
Then you need the performance metrics averaged across cross-validation folds. This includes the mean test log-likelihood (averaged over the five test folds), the standard deviation of test log-likelihoods across folds (which tells you how variable performance was across different train-test splits), the mean training log-likelihood (for comparison to test to check for overfitting), the mean accuracy on test trials, the mean accuracy on training trials, the AIC computed from the mean test log-likelihood, and the BIC computed from the mean test log-likelihood.
You should also include computational metadata for each run. This includes the wall-clock time in seconds that the fitting procedure took for this run, the number of grid points evaluated (which might vary if you implement any early stopping or adaptive strategies), and the final best log-likelihood achieved on training data (to verify that the grid search found good parameter values). If you're tracking memory usage or GPU utilization, you could include those as well, though for CPU-based grid search this is less critical.
Finally, include some run-specific summary statistics that might be useful for diagnostics. This could include the number of reversals the generator experienced in this run (should be constant if you're using a fixed reversal schedule, but good to verify), the proportion of trials where the generator chose each action type (to check that the generators are producing reasonable behavior distributions), and the mean belief entropy across trials (for the active inference generators, to verify they're actually uncertain after reversals and confident during stable periods).
Fold-Level Data Files
Within each run, you have five cross-validation folds, and it's useful to save fold-specific information to verify that your cross-validation is working correctly and that performance is consistent across folds. For each generator-model-run combination, you could save a fold-level CSV with five rows, named something like "fold_data_gen_M3_model_M1_run_005.csv".
Each row represents one fold and should contain the fold index (zero to four), the indices of trials that were in the training set for this fold, the indices of trials that were in the test set for this fold, the fitted parameters found using this fold's training data, the training log-likelihood achieved with these parameters, the test log-likelihood on this fold's held-out data, the test accuracy on this fold's held-out data, and the number of grid evaluations performed during fitting for this fold.
The value of fold-level data is that it lets you diagnose potential problems with cross-validation. For example, if one fold consistently achieves much worse test log-likelihood than the others across all models, this might indicate that the test set for that fold contains unusually difficult trials (perhaps it includes a reversal point). Or if the fitted parameters vary wildly across folds, this might indicate that the parameter space has multiple local optima or that your grid isn't fine enough to pin down a unique solution.
Aggregate Confusion Matrices
At the highest level of aggregation, you need the confusion matrices themselves. These should be saved as CSV files where rows represent generators and columns represent fitted models. You should create separate confusion matrix files for each metric: log-likelihood, accuracy, AIC, and BIC.
For the log-likelihood confusion matrix, each cell contains the mean cross-validated test log-likelihood across the twenty runs, and you should also save a companion file with standard errors for each cell. The log-likelihood confusion matrix is your primary result because it directly measures predictive accuracy without any model complexity adjustments.
For the AIC confusion matrix, each cell contains the mean AIC across twenty runs, computed from the mean test log-likelihoods and the known parameter counts for each model. Lower AIC is better, so you're looking for which models achieve the lowest values on each row.
For the BIC confusion matrix, each cell contains the mean BIC across twenty runs. BIC penalizes complexity more strongly than AIC, so if M3 has lower BIC than M1 on M1-generated data despite having more parameters, this is strong evidence that M3's improved fit is meaningful.
For the accuracy confusion matrix, each cell contains the mean test accuracy across twenty runs. This is supplementary to the log-likelihood results because accuracy only looks at whether the model chose the optimal action, not how confident it was in that choice.
Each confusion matrix file should include row and column labels clearly indicating which generator and model each entry corresponds to, and should have a header row explaining the metric being reported. You should also save metadata files that document the experimental parameters used: the number of runs per generator, the number of trials per run, the reversal schedule, the cross-validation fold structure, the parameter grid specifications for each model, and the date and time the experiment was conducted.
Parameter Landscape Data
One additional type of output that could be valuable is the full grid evaluation results, not just the best parameters. For each generator-model-run-fold combination, if storage space permits, you could save a file containing every parameter combination you evaluated and the training log-likelihood achieved by each. This would be a larger dataset (potentially millions of rows across all combinations) but would allow you to visualize the parameter landscape and understand how sharply peaked the likelihood surface is.
For example, you might discover that M1's gamma parameter has a very flat likelihood surface around its optimal value, meaning many different gamma values achieve similar fit. Or you might find that M3's profile parameters interact in complex ways where certain combinations work well but interpolations between them work poorly. This information could inform future studies about which parameters are most important to get right and which ones are less critical.
You could structure this as one CSV per generator-model-fold combination (so five generators times three models times five folds equals seventy-five files if you're not distinguishing runs), with each row containing a parameter combination and its training log-likelihood. The columns would be the parameter values (gamma, preference logits, etc.) and the log-likelihood. If this becomes too much data to store, you could subsample by only saving every tenth parameter combination or only saving combinations that achieved log-likelihood within some threshold of the best value.
Metadata and Configuration Files
Finally, you should save comprehensive metadata about your experimental setup so that your results are fully reproducible. Create a JSON or YAML configuration file that documents every setting you used: the generative model parameters for each generator (the specific gamma, preference logit, and other values), the parameter grids you searched over for each model during fitting (the exact values sampled for each parameter), the cross-validation structure (number of folds, how trials were split), the computational environment (Python version, pymdp version, numpy version, number of CPU cores used), and any other settings that could affect the results.
You should also save a log file from your experimental run that records when each stage started and finished, any warnings or errors that occurred, and summary statistics like total CPU hours consumed. This helps verify that the experiment completed successfully and provides context for interpreting computational cost claims in your paper.
Why This Level of Detail Matters
The reason I'm recommending such comprehensive data output is that model recovery experiments often reveal unexpected patterns that require follow-up investigation. You might find that M3 fits M1-generated data almost as well as M1 itself, and you'll want to investigate why. With trial-level data, you can examine whether M3 is using both profiles equally (effectively collapsing to single-profile behavior) or whether it's switching between profiles more than the M1 generator did. With fitted parameter data, you can check whether M3's profile parameters converged to similar values when fitting M1 data, which would explain the similar performance.
Or you might find that model recovery works perfectly on M3-generated data but poorly on M2-generated data, with multiple models achieving similar fit. With fold-level data, you can check whether this is consistent across cross-validation folds or whether certain folds are driving the ambiguity. With parameter landscape data, you can visualize whether M2 data produces a flat likelihood surface where many models work equally well.
The trial-level data is especially valuable because it lets you perform mechanistic validation similar to what you did in your main paper. You can plot belief trajectories around reversals for fitted models and compare them to the generator's beliefs. You can compute profile recruitment patterns for M3 when fitted to different generators' data and see whether it's using state-conditional control appropriately. These mechanistic analyses are what will distinguish your work from purely statistical model comparison and connect the recovery results back to your theoretical claims about adaptive precision control.
Practical Implementation Notes
When implementing this data output structure, you should create a clear directory hierarchy to keep files organized. Something like "results/model_recovery/trial_level/gen_M3/model_M1/run_005.csv" ensures you can navigate to specific results quickly. Use consistent naming conventions throughout so that scripts can automatically discover and load files based on their names.
Write helper functions for saving each type of output file so you don't have to reconstruct column names and format strings repeatedly. These helper functions should include validity checks, like verifying that trial indices are sequential, that cross-validation folds don't overlap, and that log-likelihood values are finite. Catching data issues during the experiment is much easier than diagnosing them afterward.
Consider implementing periodic checkpointing where you save intermediate results as they complete rather than waiting until all runs finish. This way if something crashes after fifty runs, you haven't lost all that computation. You could save run-level and trial-level data as each run completes, then aggregate them into confusion matrices at the end.
Finally, create a summary script that reads your detailed outputs and generates human-readable summary reports. This script should produce the confusion matrices, compute statistical comparisons (like paired t-tests between models on each generator), identify which model wins on each generator and by how much, and flag any concerning patterns like fitted parameters hitting grid boundaries or fold-level performance being highly variable. Running this summary script immediately after your experiment completes will help you quickly assess whether the results make sense and whether any follow-up analyses are needed.
Does this specification give you everything you need to implement the data outputs? Would you like me to help you write the actual code for saving these different data types, or would you like to discuss any aspects of the experimental protocol in more detail before we proceed?