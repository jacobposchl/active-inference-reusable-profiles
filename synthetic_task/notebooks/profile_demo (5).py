# -*- coding: utf-8 -*-
"""profile_demo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ji6D9GkqrlmUC5CpwdeVxpOEjAIRoVpi

Based off the pymdp package. Much of this code is reused from the "Tutorial 2" file.

@article{Heins2022,
  doi = {10.21105/joss.04098},
  url = {https://doi.org/10.21105/joss.04098},
  year = {2022},
  publisher = {The Open Journal},
  volume = {7},
  number = {73},
  pages = {4098},
  author = {Conor Heins and Beren Millidge and Daphne Demekas and Brennan Klein and Karl Friston and Iain D. Couzin and Alexander Tschantz},
  title = {pymdp: A Python library for active inference in discrete state spaces},
  journal = {Journal of Open Source Software}
}

# Imports and packages
### Necessary packages
"""

! pip install inferactively-pymdp -q

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pymdp
from pymdp import utils
from pymdp.maths import softmax
from pymdp.agent import Agent

"""# Graphs
### Plots so we can visualize the results
"""

#So we can visualize the likelihood matrix A
def plot_likelihood(matrix, title_str = "Likelihood distribution (A)"):
    """
    Plots a 2-D likelihood matrix as a heatmap
    """

    if not np.isclose(matrix.sum(axis=0), 1.0).all():
      raise ValueError("Distribution not column-normalized! Please normalize (ensure matrix.sum(axis=0) == 1.0 for all columns)")

    fig = plt.figure(figsize = (6,6))
    ax = sns.heatmap(matrix, cmap = 'gray', cbar = False, vmin = 0.0, vmax = 1.0)
    plt.title(title_str)
    plt.show()

#To visualize the current q(s)
def plot_beliefs(belief_dist, title_str=""):
    """
    Plot a categorical distribution or belief distribution, stored in the 1-D numpy vector `belief_dist`
    """

    if not np.isclose(belief_dist.sum(), 1.0):
      raise ValueError("Distribution not normalized! Please normalize")

    plt.grid(zorder=0)
    plt.bar(range(belief_dist.shape[0]), belief_dist, color='r', zorder=3)
    plt.xticks(range(belief_dist.shape[0]))
    plt.title(title_str)
    plt.show()


#Allows us to visualize the gamma over time, with reversal markings and smoothing display
def plot_gamma_over_time(gamma_series, reversals=None, belief_left_series=None, roll_k=None, title="Policy precision (γ) over trials", ylabel="γ (policy precision)", show = True):
  """
  We input the gamma_series, which is the gamma per trial, which we plot with the graph
  We can set the reversals, where we encode the trial indices of the reversals as to display
  We can encode a rolling mean, as to smooth the display for better visualization of trends


  """
  #Total time of the trials
  T = len(gamma_series)
  t = np.arange(T)


  #Here, we encode the gamma_series as an array of floats, in which we forward fill missing values
  gamma = np.asarray(gamma_series, dtype=float)

  if np.isnan(gamma).any():
    # forward fill
    for i in range(1, T):
      #if there's a valid gamma from the previous time stamp
      if np.isnan(gamma[i]) and not np.isnan(gamma[i-1]):
        gamma[i] = gamma[i-1]
    # back-fill from first valid
    first_valid = np.flatnonzero(~np.isnan(gamma))
    if first_valid.size:
        gamma[:first_valid[0]] = gamma[first_valid[0]]
    else:
        gamma[:] = 0.0

  #encode our smoothed gamma to plot
  gamma_smooth = rolling_mean(gamma, roll_k)

  fig, ax = plt.subplots(figsize=(9, 4.5))
  ax.plot(t, gamma, lw=1, alpha=0.35, label="γ (raw)")
  if roll_k and roll_k > 1:
    ax.plot(t, gamma_smooth, lw=2, label=f"γ (rolling mean, k={roll_k})")

  ax.set_xlabel("trial")
  ax.set_ylabel(ylabel)
  ax.set_title(title)

  #display the reversals as vertical dotted lines
  if reversals:
    for r in reversals:
      ax.axvline(r, linestyle="--", linewidth=1, alpha=0.7)
      # add a legend hint
      ax.plot([], [], linestyle="--", color=ax.lines[0].get_color(), label="reversal")

  ax.grid(True, alpha=0.3)
  fig.tight_layout()
  if show:
    plt.show()
  return fig, ax

#We can plot the uncertainty of the q(s) over time -> H(q(s))
def plot_entropy_over_time(belief_series, reversals=None, roll_k=None, title="Belief entropy over trials", ylabel="Entropy H(q(s))", show = True):
  """
  We plot our q(s) from t -> T in belief series
  We can also display the reversals for better visualization
  We dislpay a rolling mean, for easier interpretation

  """

  #total trials
  T = len(belief_series)

  # compute entropy per trial
  entropies = np.array([compute_entropy(q) for q in belief_series])

  #smoothing
  if roll_k and roll_k > 1:
    kernel = np.ones(roll_k) / roll_k
    ent_smooth = np.convolve(entropies, kernel, mode="same")
  else:
    ent_smooth = entropies


  fig, ax = plt.subplots(figsize=(9, 4))
  ax.plot(entropies, color="gray", lw=1, alpha=0.5, label="raw")
  if roll_k and roll_k > 1:
    ax.plot(ent_smooth, color="red", lw=2, label=f"rolling mean (k={roll_k})")

  ax.set_xlabel("Trial")
  ax.set_ylabel(ylabel)
  ax.set_title(title)
  ax.grid(True, alpha=0.3)

  # mark reversals as dotted vertical lines
  if reversals:
    for r in reversals:
        ax.axvline(r, linestyle="--", color="black", alpha=0.6)
    ax.plot([], [], "--", color="black", label="reversal")

  ax.legend(loc="upper right")
  fig.tight_layout()
  if show:
    plt.show()
  return fig, ax

def plot_switch_dynamics(choices, true_contexts, window=10, title="Switch Dynamics", show=True):
  """
  Plot mean accuracy around reversal points.

  Parameters:
  -----------
  choices : list of str
      Action labels taken at each trial (e.g., 'act_left', 'act_right')
  true_contexts : list of str
      True context at each trial (e.g., 'left_better', 'right_better')
  window : int
      Number of trials before/after reversals to include
  title : str
      Plot title
  show : bool
      Whether to display the plot

  Returns:
  --------
  fig, ax : matplotlib figure and axis objects
  """

  choices = np.array(choices)
  contexts = np.array(true_contexts)
  T = len(choices)


  # Find the reversals in the trials
  # A reversal occurs when context[t] != context[t-1]
  reversals = np.where(contexts[1:] != contexts[:-1])[0] + 1


  if len(reversals) == 0:
    print("No reversals detected, nothing to plot.")
    return None, None

  # Compute correctness at each trial
  # Correct if: (context is left_better AND action contains 'left') OR
  #             (context is right_better AND action contains 'right')
  correct = np.zeros(T, dtype=float)
  for t in range(T):
    c = contexts[t]
    a = choices[t]

    # Check if action is correct for current context
    if c == 'left_better' and 'left' in a:
      correct[t] = 1.0
    elif c == 'right_better' and 'right' in a:
      correct[t] = 1.0
    else:
      correct[t] = 0.0

  # Encode accuracies around each reversal
  rel_acc = []
  for r in reversals:
    start = max(0, r - window)
    end   = min(T, r + window)

    # Extract segment
    seg = correct[start:end]

    # Pad to full length if edges are short
    pad_left  = window - (r - start)
    pad_right = window - (end - r)
    seg_padded = np.pad(seg, (pad_left, pad_right), constant_values=np.nan)

    rel_acc.append(seg_padded)

  # Convert the list of 1D arrays into a 2D array
  # Rows -> number of reversals
  # Cols -> length of each segment (2*window)
  rel_acc = np.vstack(rel_acc)  # shape: (n_reversals, 2*window)

  # Compute mean accuracy across reversals (ignoring NaNs)
  mean_acc = np.nanmean(rel_acc, axis=0)

  # X-axis: trials relative to reversal
  x = np.arange(-window, window)

  # Plot
  fig, ax = plt.subplots(figsize=(8, 4))
  ax.plot(x, mean_acc, lw=2, color='dodgerblue', marker='o', markersize=4)
  ax.axvline(0, linestyle='--', color='k', alpha=0.7, label='Reversal')
  ax.set_xlabel("Trials relative to reversal")
  ax.set_ylabel("Mean accuracy")
  ax.set_ylim(-0.05, 1.05)
  ax.set_title(title)
  ax.grid(alpha=0.3)
  ax.legend()
  fig.tight_layout()

  if show:
    plt.show()

  return fig, ax


""" HELPER FUNCTION """
#calculate the rolling mean
def rolling_mean(x, k):
    if k is None or k <= 1:
        return np.asarray(x, dtype=float)
    x = np.asarray(x, dtype=float)
    # pad at ends for centered window
    pad = k // 2
    xpad = np.pad(x, (pad, pad), mode='edge')
    cumsum = np.cumsum(xpad)
    # centered rolling mean
    out = (cumsum[k:] - cumsum[:-k]) / float(k)
    # if k is even, shift to align with center-ish
    if len(out) < len(x):
        out = np.r_[out, np.repeat(out[-1], len(x) - len(out))]
    return out[:len(x)]

#Compute the entropy of a probability distribution
def compute_entropy(probs):
    #Clipped to handle log(0)
    p = np.clip(np.asarray(probs, dtype=float), 1e-12, 1.0)
    return -np.sum(p * np.log(p))

# Add this function near your other helper functions (after compute_entropy)

def print_trial_details(t, env, qs, gamma_t, action_label, obs_labels, show=True):
    """
    Print detailed information about what happened in a trial.

    Parameters:
    -----------
    t : int
        Trial number
    env : TwoArmedBandit
        The environment (to get true context)
    qs : list of arrays
        Posterior beliefs over hidden states
    gamma_t : float
        Policy precision this trial
    action_label : str
        Action taken this trial
    obs_labels : list of str
        Observations received [hint_obs, reward_obs, choice_obs]
    show : bool
        Whether to print (allows conditional printing)
    """
    if not show:
        return

    # Extract belief over context (factor 0)
    q_context = qs[0]  # [p(left_better), p(right_better)]

    # Extract belief over choice state (factor 1)
    q_choice = qs[1]  # [p(start), p(hint), p(left), p(right)]

    # Compute entropy of context belief
    H_context = compute_entropy(q_context)

    # Determine if action was correct
    correct = "✓" if ((env.context == 'left_better' and 'left' in action_label) or
                      (env.context == 'right_better' and 'right' in action_label)) else "✗"

    #print(f"t={t:3d} | True:{env.context:12s} | Action:{action_label:12s} {correct} | " f"Reward:{obs_labels[1]:14s} | " f"q(left)={q_context[0]:.3f} | H={H_context:.3f} | γ={gamma_t:.3f}")

"""# Dimensionalities
### Initialization of s_t, o_t, and a_t dimensionalities
"""

""" DEFINE THE HIDDEN STATE """
#We have two hidden state factors
# 1. Hidden State Contexts -> the underlying distribution of the generative process in which the left or right arm is better
# 2. Hidden State Choices -> which state we're currently in, as to guide the observations we receive

state_contexts = ['left_better', 'right_better'] #Could later be changed in PRL task, where the hidden context about the environment is swapped
state_choices = ['start', 'hint', 'left', 'right'] #The different choices we could be in right now

#We have two different factors for the Hidden State, and each factor has their own set of possible states
num_states = [len(state_contexts), len(state_choices)]
num_factors = len(num_states)



""" DEFINE THE ACTIONS """
action_contexts = ['rest'] #The action assigned to the hidden context states, we can't do anything about the hidden context, so we just "rest"
action_choices = ['act_start', 'act_hint', 'act_left', 'act_right'] #We assign an action for each hidden choices
#note, the action choices directly transition our state choice from time t-1 to t in the transition matrix (B)

#We have two different factors for Actions
num_actions = [len(action_contexts), len(action_choices)]



""" DEFINE THE OBSERVATIONS """
#We have three observation modalities
# 1. Hint Observation -> we observe a hint either being null (for cases in which we're not in the hint state), left hint or right hint
# 2. Reward Observation -> we observe a reward either null (for cases in which we're in the start or hint state), loss or reward, depending on the latent context
# 3. Choice Observation -> one-to-one mapping of the current state choice
observation_hints = ['null', 'observe_left_hint', 'observe_right_hint'] #Different possible hint observations we could have
observation_rewards = ['null', 'observe_loss', 'observe_reward'] #Different possible observation rewards we could recieve
observation_choices = ['observe_start', 'observe_hint', 'observe_left', 'observe_right'] #Different possible observations about choices we could have


num_obs = [len(observation_hints), len(observation_rewards), len(observation_choices)]
num_modalities = len(num_obs)



probability_hint = 0.7
probability_reward = 0.8


trials = 200

"""# Generative Model

### A Matrix - Likelihood
"""

#Function to initialize the A matrix
# p(o_t | s_t, a_t)
def build_A(num_modalities, state_contexts, state_choices, observation_hints, observation_rewards, observation_choices, p_hint, p_reward):
  # modalities of observations -> [hint observations, reward observations, choice observations]
  A = utils.obj_array( num_modalities )

  # p(o[0]) -> hint observation
  A_hint = np.zeros((len(observation_hints), len(state_contexts), len(state_choices)))

  # the state choices -> [start, hint, left, right]
  for choice_index, choice in enumerate(state_choices):
    #depending on the current state the agent is in, the hint observation will be 0 for cases where state is not in hint
    if choice == "start":
      A_hint[0, :, choice_index] = 1

    elif choice == "hint":
      A_hint[1, :, choice_index] = [probability_hint, 1 - probability_hint]
      A_hint[2, :, choice_index] = [1 - probability_hint, probability_hint]

    elif choice == "left":
      A_hint[0, :, choice_index] = 1

    elif choice == "right":
      A_hint[0, :, choice_index] = 1


  # p(o[1]) -> reward observation
  A_reward = np.zeros((len(observation_rewards), len(state_contexts), len(state_choices)))

  for choice_index, choice in enumerate(state_choices):
    #if the agent state is either in start or hint, it gets no reward
    if choice == "start":
      A_reward[0, :, choice_index] = 1

    elif choice == "hint":
      A_reward[0, :, choice_index] = 1

    elif choice == "left":
      A_reward[1, :, choice_index] = [1 - probability_reward, probability_reward]
      A_reward[2, :, choice_index] = [probability_reward, 1 - probability_reward]

    elif choice == "right":
      A_reward[1, :, choice_index] = [probability_reward, 1 - probability_reward]
      A_reward[2, :, choice_index] = [1 - probability_reward, probability_reward]


  # p(o[2]) -> choice observation
  A_choice = np.zeros((len(observation_choices), len(state_contexts), len(state_choices)))

  for choice_index, choice in enumerate(state_choices):
    #observation of the agent choice will be that of which the agent is in with full certainty
    A_choice[choice_index, :, choice_index] = 1



  A[0] = A_hint
  A[1] = A_reward
  A[2] = A_choice
  return A

"""### B Matrix - Transition"""

def build_B(state_contexts, state_choices, action_contexts, action_choices, context_volatility=0.0):
  """
  Build the transition matrix B that defines p(s_t | s_t-1, a_t-1).

  Parameters:
  -----------
  state_contexts : list of str
      Context state names (e.g., ['left_better', 'right_better'])
  state_choices : list of str
      Choice state names (e.g., ['start', 'hint', 'left', 'right'])
  action_contexts : list of str
      Actions for context factor (e.g., ['rest'])
  action_choices : list of str
      Actions for choice factor (e.g., ['act_start', 'act_hint', 'act_left', 'act_right'])
  context_volatility : float, default=0.0
      Probability that the context switches on any given trial.
      0.0 = contexts never change (agent assumes stable world)
      0.1 = 10% chance context flips each trial (agent expects some volatility)
      Higher values make the agent more ready to switch beliefs after reversals.

  Returns:
  --------
  B : object array of transition matrices
      B[0] = context transitions, B[1] = choice transitions
  """
  B = utils.obj_array(2)  # Two factors: context and choice

  # B[0]: Transition dynamics for CONTEXT states
  # The hidden environment context (left_better / right_better)
  # Actions don't affect context, so we have one transition matrix for action "rest"
  B_context = np.zeros((len(state_contexts), len(state_contexts), len(action_contexts)))

  # Build transition matrix with optional volatility
  # Volatility represents the agent's belief about how often contexts change
  if context_volatility == 0.0:
    # No volatility: contexts are perfectly stable
    # If context was left_better, it stays left_better with p=1.0
    B_context[:,:,0] = np.eye(len(state_contexts))
  else:
    # With volatility: contexts can flip
    # If context was left_better:
    #   - Stay left_better with p = (1 - volatility)
    #   - Switch to right_better with p = volatility
    # This makes the agent more willing to update beliefs when evidence changes
    for i in range(len(state_contexts)):
      for j in range(len(state_contexts)):
        if i == j:
          # Probability of staying in same context
          B_context[i, j, 0] = 1.0 - context_volatility
        else:
          # Probability of switching to other context
          B_context[i, j, 0] = context_volatility


  # B[1]: Transition dynamics for CHOICE states
  # The agent's current position in the task (start, hint, left, right)
  # Actions have a deterministic one-to-one mapping to next state
  B_choice = np.zeros((len(state_choices), len(state_choices), len(action_choices)))

  for choice_index in range(len(state_choices)):
    # Full one-to-one mapping of chosen action and future choice state
    # If agent takes act_left, it deterministically transitions to state 'left'
    B_choice[choice_index, :, choice_index] = 1.0

  B[0] = B_context
  B[1] = B_choice
  return B

"""### D vector - Prior"""

#Function to initialize the D vector
# p(s_t=1)
def build_D(state_contexts, state_choices):
    D = utils.obj_array(num_factors)

    #Uniform probability distribution of starting context state
    #Agent weights equal probability of left or right being the better arm
    D_context = np.array([0.5,0.5])

    D_choice = np.zeros(len(state_choices))

    D_choice[state_choices.index("start")] = 1.0

    D[0] = D_context
    D[1] = D_choice

    return D

"""# Environment
#### Here we define what the observations we receive from the environment are, given inputs
"""

#This is the generative process in which we interact with the environment by giving an input (action) and receiving an observation
#In this case, we are using the Two Armed Bandit environment
class TwoArmedBandit(object):
  def __init__(self, context = None, probability_hint = 1, probability_reward = 0.8, reversal_schedule = None):

    self.context_names = ['left_better', 'right_better']

    # sample context if not provided
    if context is None:
        self.context = self.context_names[utils.sample(np.array([0.5, 0.5]))]
    else:
        assert context in self.context_names
        self.context = context

    #correctness of the hint when requested
    self.probability_hint = probability_hint

    #win rate when pulling the better arm
    self.probability_reward = probability_reward

    self.observation_hints = observation_hints
    self.observation_rewards = observation_rewards

    self.reversal_schedule = reversal_schedule or []
    self.trial_count = 0

  #step function where given a specific action, we are returned the observations (3 modalities) in which is fed into the agent
  def step(self, action):

    #enact reversals given reversal scheduler
    if self.trial_count in self.reversal_schedule:
      self.context = 'right_better' if self.context == 'left_better' else 'left_better'
      #print(f"REVERSAL at trial {self.trial_count}: context now {self.context}")

    self.trial_count += 1


    if action == "act_start":
      observed_hint = "null"
      observed_reward = "null"
      observed_choice = "observe_start"
    elif action == "act_hint":
      if self.context == "left_better":
        observed_hint = self.observation_hints[utils.sample(np.array([0.0, self.probability_hint, 1.0 - self.probability_hint]))]
      elif self.context == "right_better":
        observed_hint = self.observation_hints[utils.sample(np.array([0.0, 1.0 - self.probability_hint, self.probability_hint]))]
      observed_reward = "null"
      observed_choice = "observe_hint"
    elif action == "act_left":
      observed_hint = "null"
      if self.context == "left_better":
        observed_reward = self.observation_rewards[utils.sample(np.array([0.0, 1.0 - self.probability_reward, self.probability_reward]))]
      elif self.context == "right_better":
        observed_reward = self.observation_rewards[utils.sample(np.array([0.0, self.probability_reward, 1.0 - self.probability_reward]))]
      observed_choice = "observe_left"
    elif action == "act_right":
      observed_hint = "null"
      if self.context == "left_better":
        observed_reward = self.observation_rewards[utils.sample(np.array([0.0, self.probability_reward, 1.0 - self.probability_reward]))]
      elif self.context == "right_better":
        observed_reward = self.observation_rewards[utils.sample(np.array([0.0, 1.0 - self.probability_reward, self.probability_reward]))]
      observed_choice = "observe_right"

    observations = [observed_hint, observed_reward, observed_choice]

    return observations

"""# Values
#### We define the profiles created based on each model configuration

### Model - Static Global
"""

#Function that returns the value function given C logits and E logits
#Model 1 has no state or time dependence
#We leave E logits as optional as for ablation tests
def make_values_M1(C_reward_logits = None, gamma = 1.0, E_logits = None):

  C_logits = np.asarray(C_reward_logits, float)
  E_logits = None if E_logits is None else np.asarray(E_logits, float)

  #value function to return the C vector, E vector, and gamma precision given the current time and state context
  def value_fn(q_context_t, t):
    C_t = softmax(C_logits)
    E_t = None if E_logits is None else softmax(E_logits)

    return C_t, E_t, float(gamma)

  return value_fn

"""### Model 2 - Dynamic Global Precision Without Profiles"""

#Function that returns the value function given C logits a gamma schedule and E logits
#Model 2 has fixed C and E vectors, but gamma is based on the current agent context belief
def make_values_M2(C_reward_logits = None, gamma_schedule = None, E_logits = None):

  if gamma_schedule is None:
    # H high -> gamma low
    # H low -> gamma high
    # Under uncertainty, we soften the policy selection
    def gamma_schedule(q_context_t, t, g_base=1.5, k=1.0):
        p = np.clip(np.asarray(q_context_t, float), 1e-12, 1.0)
        H = -(p*np.log(p)).sum()
        return g_base / (1.0 + k*H)

  #C and E vectors are fixed across trials
  C_logits = np.asarray(C_reward_logits, float)
  E_logits = None if E_logits is None else np.asarray(E_logits, float)

  def value_fn(q_context_t, t):
    C_t = softmax(C_logits)
    E_t = None if E_logits is None else softmax(E_logits)
    gamma_t = float(gamma_schedule(q_context_t, t))

    return C_t, E_t, gamma_t

  return value_fn

"""### Model 3 - Profile Model"""

def make_values_M3(profiles, Z, num_policies=None, policies=None, num_actions_per_factor=None):
  """
  Creates a value function for Model 3 (Profile Model) that mixes value profiles
  based on current beliefs about hidden states.

  Parameters:
  -----------
  profiles : list of dict
      Each profile contains:
      - 'phi_logits': Log preferences over outcomes (will become C vector after softmax)
      - 'xi_logits': Log preferences over ACTIONS (will be converted to policy-level E vector)
      - 'gamma': Policy precision (decisiveness) scalar
  Z : array-like, shape (num_states, num_profiles)
      Assignment matrix linking hidden states to profiles.
      Each row sums to 1.0 and represents how much each profile is used for that state.
  policies : list of arrays (required if using xi_logits)
      The complete list of policies generated by pymdp.
      Each policy is a [policy_len x num_factors] array of action indices.
  num_actions_per_factor : list (required if using xi_logits)
      Number of actions available for each control factor.
      Example: [1, 4] means factor 0 has 1 action, factor 1 has 4 actions.

  Returns:
  --------
  value_fn : function
      A function that takes (q_context_t, t) and returns (C_t, E_t, gamma_t)
      where the values are belief-weighted mixtures of the profiles.
  """

  # Normalize Z so each row (state) sums to 1.0
  # This ensures we have valid probability distributions over profiles
  Z = np.asarray(Z, float)
  Z /= Z.sum(axis=1, keepdims=True)

  # Extract and prepack profile parameters for efficient computation
  K = len(profiles)  # Number of profiles (e.g., 2: explore-ish, exploit-ish)

  # PHI: Outcome preference logits for each profile
  # Shape: [K profiles, O outcomes]
  PHI = np.stack([np.asarray(p['phi_logits'], float) for p in profiles], axis=0)

  # GAM: Policy precision (gamma) for each profile
  # Shape: [K profiles]
  # Higher gamma = more decisive/exploitative, lower gamma = more exploratory
  GAM = np.array([float(p['gamma']) for p in profiles])

  # Check if all profiles have policy priors (xi_logits)
  has_xi = all('xi_logits' in p for p in profiles)

  if has_xi:
    # XI will store policy-level preference logits (not action-level)
    # We need to convert from action preferences to policy preferences

    # Ensure we have the necessary information to do the conversion
    if policies is None or num_actions_per_factor is None:
      raise ValueError("Must provide 'policies' and 'num_actions_per_factor' when using xi_logits")

    # Convert each profile's ACTION-level preferences to POLICY-level preferences
    # This is necessary because:
    # - xi_logits has length 4 (one per action: start, hint, left, right)
    # - But pymdp needs preferences over all 16 policies (2-step action sequences)
    XI_list = []
    for p in profiles:
      # Map action preferences to policy preferences
      # Example: if xi_logits favors 'act_left', then policies containing 'act_left'
      # will receive higher preference
      policy_logits = map_action_prefs_to_policy_prefs(
        p['xi_logits'],           # Action-level preferences [4]
        policies,                  # All possible policies [16]
        num_actions_per_factor     # [1, 4] in our case
      )
      XI_list.append(policy_logits)

    # Stack into array: Shape [K profiles, num_policies]
    XI = np.stack(XI_list, axis=0)
  else:
    # No policy priors specified - will return None for E_t
    XI = None

  # Define the value function that will be called at each trial
  def value_fn(q_context_t, t):
    """
    Compute trial-specific value parameters by mixing profiles according to beliefs.

    Parameters:
    -----------
    q_context_t : array-like, shape (num_context_states,)
        Current posterior belief over context states (e.g., [p(left_better), p(right_better)])
    t : int
        Current trial number (not used in M3, but kept for API consistency)

    Returns:
    --------
    C_t : array, shape (num_outcomes,)
        Trial-specific outcome preferences (probability distribution)
    E_t : array, shape (num_policies,) or None
        Trial-specific policy priors (probability distribution), or None if not using xi
    gamma_t : float
        Trial-specific policy precision (decisiveness parameter)
    """

    # Step 1: Compute profile weights from current beliefs
    # w[k] = how much profile k should influence behavior right now
    # This is computed as: belief over states (q_context_t) @ assignment matrix (Z)
    # Example: if q_context_t = [0.9, 0.1] (90% believe left_better)
    #          and Z = [[1, 0], [0, 1]] (hard assignment)
    #          then w = [0.9, 0.1] (mostly use profile 0)
    w = np.asarray(q_context_t, float) @ Z  # Shape: [K]

    # Normalize weights to sum to 1 (handle numerical issues)
    w = w / (w.sum() + 1e-12)

    # Step 2: Mix outcome preference logits across profiles
    # phi_t is a weighted average of each profile's phi_logits
    # w[:,None] broadcasts weights to multiply each profile's full logit vector
    # Shape: [K, 1] * [K, O] -> sum over K -> [O]
    phi_t = (w[:,None] * PHI).sum(axis=0)

    # Step 3: Mix policy precisions across profiles
    # gamma_t is a weighted average of each profile's gamma
    # Shape: [K] * [K] -> sum -> scalar
    gamma_t = float((w * GAM).sum())

    # Step 4: Convert outcome logits to probability distribution
    # Softmax ensures C_t sums to 1 and represents valid preferences
    C_t = softmax(phi_t)

    # Step 5: Mix and convert policy preference logits (if available)
    if XI is not None:
      # Mix policy preference logits across profiles
      # Shape: [K, 1] * [K, num_policies] -> sum over K -> [num_policies]
      xi_t = (w[:, None] * XI).sum(axis=0)

      # Convert to probability distribution over policies
      E_t = softmax(xi_t)
    else:
      # No policy priors - let pymdp use its default (uniform or no prior)
      E_t = None

    return C_t, E_t, gamma_t

  # Return the configured value function
  return value_fn

# Helper function to map action preferences to policy preferences
def map_action_prefs_to_policy_prefs(xi_logits_per_action, policies, num_actions_per_factor):
    """
    Map action-level preferences (xi_logits) to policy-level preferences (E).

    Parameters:
    -----------
    xi_logits_per_action : array-like, shape (num_actions,)
        Log preferences for each action type
    policies : list of arrays
        The policies generated by pymdp. Each policy is a [T x num_factors] array
    num_actions_per_factor : list
        Number of actions for each control factor

    Returns:
    --------
    policy_logits : np.ndarray, shape (num_policies,)
        Log preferences for each policy
    """
    xi = np.asarray(xi_logits_per_action, dtype=float)
    num_policies = len(policies)
    policy_logits = np.zeros(num_policies)

    for pol_idx, policy in enumerate(policies):
        # policy shape: [policy_len, num_control_factors]
        # For your case: [2, 2] but only factor 1 (choice) is controllable

        # Sum log preferences across timesteps for this policy
        log_pref = 0.0
        for t in range(policy.shape[0]):
            action_idx = int(policy[t, 1])  # Factor 1 is the choice factor
            log_pref += xi[action_idx]

        policy_logits[pol_idx] = log_pref

    return policy_logits

"""### Value Function"""

def make_value_fn(model, **cfg):
  """
  Factory function to create value functions for different models.

  Parameters:
  -----------
  model : str
      Model identifier: 'M1', 'M2', or 'M3'
  **cfg : dict
      Model-specific configuration parameters

  Returns:
  --------
  value_fn : function
      A function that takes (q_context_t, t) and returns (C_t, E_t, gamma_t)
  """

  # Model 1: Static global precision and preferences
  # Everything is fixed across time and states
  if model == "M1":
    return make_values_M1(**cfg)

  # Model 2: Dynamic global precision without profiles
  # Precision changes with belief entropy, but preferences stay fixed
  if model == "M2":
    return make_values_M2(**cfg)

  # Model 3: Profile model with state-conditional mixing
  # Multiple profiles mixed based on current beliefs
  if model == "M3":
    return make_values_M3(**cfg)

  # Invalid model name
  raise ValueError("model must be in M1, M2, or M3")

"""# Agent Wrapper"""

# @title
#Agent wrapper to handle the value profiles
class AgentRunner:
  def __init__(self, A, B, D, value_fn, observation_hints, observation_rewards, observation_choices, action_choices, reward_mod_idx = 1, policy_len = 2, inference_horizon = 1):
    """
    Input the likelihood (A) and transition (B) matrices and prior (D) vector for q(s_t) calculation
    Input the value function to calculate the observation preference (C) and policy preference (E) vector and gamma (y) for policy calculation q(pi_t)
    Input the labels of our modalities
    """

    self.value_fn = value_fn

    #reward channel for C vector
    self.reward_mod_idx = reward_mod_idx

    C0 = utils.obj_array_zeros([(A[m].shape[0],) for m in range(len(A))])


    self.observation_hints = observation_hints
    self.observation_rewards = observation_rewards
    self.observation_choices = observation_choices
    self.action_choices = action_choices
    self.policy_len = policy_len

    self.gamma_t = None


    self.agent = Agent(A = A, B = B, C = C0, D = D,
                       policy_len = policy_len, inference_horizon = inference_horizon,
                       control_fac_idx = [1], use_utility = True, use_states_info_gain=True,
                       action_selection="stochastic", gamma = 16)

    #pre checks
    assert len(A) == 3, "Expecting 3 modalities: hints, rewards, choice_sensed."
    assert self.reward_mod_idx in (0,1,2), "reward_mod_idx must be a valid modality index."
    #print(f"Agent initialized with {len(self.agent.policies)} policies")
    #print(f"Policy Length: {policy_len}")


  #maps observation strings to indices per modality for easier compute
  #['hint_obs','reward_obs','choice_obs']
  def obs_labels_to_ids(self, obs_labels):
    return [self.observation_hints.index(obs_labels[0]),
          self.observation_rewards.index(obs_labels[1]),
          self.observation_choices.index(obs_labels[2])]

  #maps action index to string label
  def action_id_to_label(self, chosen_action_ids):
    a_idx = int(chosen_action_ids[1])
    return self.action_choices[a_idx]

  #called each time step to infer q(s), calculate value profile, infer q(pi) and choose q(a)
  def step(self, obs_ids, t):

    #infer the hidden states of the agent, given the observations
    qs = self.agent.infer_states(obs_ids)

    #grab the context state (left better / right better)
    q_context = qs[0]

    #calculate value profile based on context and time
    C_t, E_t, gamma_t = self.value_fn(q_context, t)

    #encode the value profiles
    self.agent.C[self.reward_mod_idx] = C_t

    if E_t is not None:
      if len(E_t) == len(self.agent.policies):
        self.agent.E = E_t
      else:
        print(f"Warning: E_t length ({len(E_t)}) doesn't match policies ({len(self.agent.policies)})")

    self.gamma_t = float(gamma_t)
    self.agent.gamma = self.gamma_t

    q_pi, efe = self.agent.infer_policies()


    #sample an action from the policy
    chosen_action_ids = self.agent.sample_action()

    u_choice = int(chosen_action_ids[1])

    #convert action id to label string
    action_label = self.action_id_to_label(chosen_action_ids)
    """
    if t < 20:
      print(f"[t={t}] ctx={env.context:12s} act={action_label:10s} " f"gamma={gamma_t:5.3f}  E_top={int(np.argmax(self.agent.E))} " f"C_len={len(self.agent.C[self.reward_mod_idx])}")

      print(f"[t={t}] policy_top={np.argmax(q_pi)}  u_choice={u_choice}  label={action_label}")
    """
    return action_label, qs, q_pi, efe, self.gamma_t

# @title
def run_episode(runner, env, T = 200, verbose = False, initial_obs_labels = None, print_around_reversals=True, window=5):
  """
  Simulate a single episode with the agent in the environment.

  Parameters:
  -----------
  runner : AgentRunner
      The agent wrapper that handles inference and control
  env : TwoArmedBandit
      The environment that generates observations
  T : int
      Number of trials to run
  verbose : bool
      If True, print details for every trial
  initial_obs_labels : list of str or None
      Starting observation [hint, reward, choice]. Defaults to start state.
  print_around_reversals : bool
      If True, print detailed info around reversal points
  window : int
      How many trials before/after reversals to print details

  Returns:
  --------
  logs : dict
      Dictionary containing trial-by-trial data for analysis
  """

  # If no initial observation given, assume starting point where no hint, no reward, just start
  if initial_obs_labels is None:
    initial_obs_labels = ['null', 'null', 'observe_start']

  # Convert labels to observation ids
  obs_ids = runner.obs_labels_to_ids(initial_obs_labels)

  # Set up logging
  logs = {
      't' : [],
      'context' : [],
      'belief' : [],
      'gamma' : [],
      'action' : [],
      'reward_label' : [],
      'choice_label' : []
  }

  # Identify reversal trials for detailed printing
  reversal_trials = set(env.reversal_schedule) if env.reversal_schedule else set()

  # Iterating through each time step: agent step -> environment step
  for t in range(T):
    # Agent perceives, infers, and acts
    action_label, qs, q_pi, efe, gamma_t = runner.step(obs_ids, t)

    # Environment responds to action
    obs_labels = env.step(action_label)
    obs_ids = runner.obs_labels_to_ids(obs_labels)

    # Logging
    logs['t'].append(t)
    logs['context'].append(env.context)
    logs['belief'].append(qs[0].copy())  # Save context beliefs
    logs['gamma'].append(gamma_t)
    logs['action'].append(action_label)
    logs['reward_label'].append(obs_labels[1])
    logs['choice_label'].append(obs_labels[2])

    # Conditional printing
    if verbose:
      # Print every trial if verbose
      print_trial_details(t, env, qs, gamma_t, action_label, obs_labels, show=True)
    elif print_around_reversals:
      # Print trials around reversals
      near_reversal = any(abs(t - r) <= window for r in reversal_trials)
      if near_reversal or t < 20:  # Also show first 20 trials
        print_trial_details(t, env, qs, gamma_t, action_label, obs_labels, show=True)

  return logs

"""# Run and Logging

### Basic Test

Here is just a basic example of using the setup. We will then build on this to run multiple seeds and experiments.
"""

# @title
#sets a seed for reproducibility
np.random.seed(999)

#Models:
# 'M1' --- Static Global
# 'M2' --- Dynamic Global Precision Without Profiles
# 'M3' --- Profile Model
model_name = 'M1'

#used my model 2,
def gamma_entropy_coupled(q_context_t, t, g_base=1.6, k=1.2):
    # H high -> gamma low
    # H low -> gamma high
    # Under uncertainty, we soften the policy selection
    """Lower precision when uncertain (higher entropy)."""
    p = np.clip(np.asarray(q_context_t, float), 1e-12, 1.0)
    H = -(p * np.log(p)).sum()
    return g_base / (1.0 + k * H)

if model_name == 'M1':
    value_fn = make_value_fn('M1', C_reward_logits=[0.0, -4.0, 2.0], gamma=1.2)
elif model_name == 'M2':
    value_fn = make_value_fn('M2', C_reward_logits=[0.0, -4.0, 2.0], gamma_schedule=lambda q,t: gamma_entropy_coupled(q, t, g_base=1.6, k=1.0))
elif model_name == 'M3':
    # Define the profiles for Model 3
    profiles = [
       # Profile 0: "Uncertain explorer" - low gamma, neutral preferences
       {'phi_logits': [0.0, -2.0, 1.5], 'xi_logits': [0.0, 0.5, 0.0, 0.0], 'gamma': 0.6},

       # Profile 1: "Confident exploiter" - high gamma, strong preferences
       {'phi_logits': [0.0, -8.0, 4.0], 'xi_logits': [0.0, -1.0, 0.0, 0.0], 'gamma': 3.0},
   ]

    # Assignment matrix Z: Links hidden context states to profiles
    # Rows = context states [left_better, right_better]
    # Columns = profiles [profile_0, profile_1]
    Z = np.array([
        [0.8, 0.2],  # When context is left_better: use profile_0 (favors left)
        [0.2, 0.8]   # When context is right_better: use profile_1 (favors right)
    ])

    # STEP 1: Create a temporary agent just to get the policies
    # We need to know what policies pymdp will generate before we can create
    # the value function that maps action preferences to policy preferences
    #print("Creating temporary agent to extract policies...")

    # Build A, B, D matrices (same as before)
    A_temp = build_A(num_modalities, state_contexts, state_choices,
                     observation_hints, observation_rewards, observation_choices,
                     probability_hint, probability_reward)
    B_temp = build_B(state_contexts, state_choices, action_contexts, action_choices)
    D_temp = build_D(state_contexts, state_choices)

    # Create a dummy C vector (we'll replace it in the real agent)
    C_temp = utils.obj_array_zeros([(A_temp[m].shape[0],) for m in range(len(A_temp))])

    # Create temporary agent with same settings
    temp_agent = Agent(
        A=A_temp,
        B=B_temp,
        C=C_temp,
        D=D_temp,
        policy_len=2,           # 2-step policies
        inference_horizon=1,
        control_fac_idx=[1],    # Only factor 1 (choice) is controllable
        use_utility=True,
        use_states_info_gain=True,
        action_selection="stochastic",
        gamma=16  # Placeholder, will be overridden by value function
    )

    # Extract the policies and action structure from the temporary agent
    policies = temp_agent.policies  # List of all possible policies
    num_actions_per_factor = [len(action_contexts), len(action_choices)]  # [1, 4]

    #print(f"Extracted {len(policies)} policies from temporary agent")
    #print(f"Action structure: {num_actions_per_factor} actions per factor")

    # STEP 2: Now create the real value function with the policy information
    # This will properly convert action-level xi_logits to policy-level E vector
    value_fn = make_value_fn(
        'M3',
        profiles=profiles,
        Z=Z,
        policies=policies,
        num_actions_per_factor=num_actions_per_factor
    )

    print("Value function M3 created successfully with policy-level preferences")
else:
    raise ValueError("model_name must be 'M1','M2','M3'")


# Build environment (true world)
#uses the globals defined earlier
p_hint_env   = probability_hint
p_reward_env = probability_reward
env = TwoArmedBandit(probability_hint=p_hint_env, probability_reward=p_reward_env, reversal_schedule=[30, 60, 90, 120, 150, 180])
#print("True context at t=0:", env.context)

# Define A, B, D
A = build_A(num_modalities, state_contexts, state_choices, observation_hints, observation_rewards, observation_choices, probability_hint, probability_reward)
B = build_B(state_contexts, state_choices, action_contexts, action_choices, context_volatility=0.05)
D = build_D(state_contexts, state_choices)

# Build runner (agent wrapper)
runner = AgentRunner(A, B, D, value_fn, observation_hints, observation_rewards, observation_choices, action_choices, reward_mod_idx=1)

# Run one episode & collect logs
logs = run_episode(runner, env, T=trials, verbose=False)

# Derive convenience arrays for plots/metrics
def find_reversals(context_series):
    """Indices where context changes (trial index where new context starts)"""
    ctx = np.asarray(context_series)
    return list(np.where(ctx[1:] != ctx[:-1])[0] + 1)

def trial_accuracy(choice_labels, context_series):
    """1 if chosen arm matches current better arm, else 0"""
    acc = []
    for a, c in zip(choice_labels, context_series):
        if c == 'left_better':
            acc.append(1 if a in ('act_left', 'left') else 0)
        elif c == 'right_better':
            acc.append(1 if a in ('act_right', 'right') else 0)
        else:
            acc.append(0)
    return np.array(acc, dtype=float)

reversals = find_reversals(logs['context'])
acc = trial_accuracy(logs['action'], logs['context'])
belief_left_series = [b[0] for b in logs['belief']]  # q(context=left_better)

#print(f"Reversals at trials: {reversals}")
print(f"Mean accuracy: {acc.mean():.3f}")

# Plots
_ = plot_gamma_over_time(
    logs['gamma'],
    reversals=reversals if len(reversals) > 0 else None,
    belief_left_series=belief_left_series,
    roll_k=7,
    title=f"γ over time — {model_name}"
)

_ = plot_entropy_over_time(
    logs['belief'],
    reversals=reversals if len(reversals) > 0 else None,
    roll_k=5,
    title=f"Belief entropy — {model_name}"
)

_ = plot_switch_dynamics(
    logs['choice_label'],
    logs['context'],
    window=10,
    title=f"Switch dynamics (mean accuracy around reversals) — {model_name}"
)

"""### Model Comparison Test"""

# @title
"""
Model Comparison Test
Compares M1 (static-global), M2 (dynamic-global), and M3 (profile model)
using action log-likelihood as the primary metric.
"""

import numpy as np
from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm.notebook import tqdm

class AgentRunnerWithLL(AgentRunner):
    """Extended AgentRunner that tracks log-likelihoods."""

    def step_with_ll(self, obs_ids, t):
        """
        Enhanced step that returns log-likelihood of chosen action.

        Returns:
        --------
        action_label : str
        qs : list of arrays
        q_pi : array
        efe : array
        gamma_t : float
        log_likelihood : float
            Log-probability of the action actually taken
        """
        # Standard inference and control
        qs = self.agent.infer_states(obs_ids)
        q_context = qs[0]

        # Get value profile
        C_t, E_t, gamma_t = self.value_fn(q_context, t)

        # Set agent parameters
        self.agent.C[self.reward_mod_idx] = C_t
        if E_t is not None:
            if len(E_t) == len(self.agent.policies):
                self.agent.E = E_t

        self.gamma_t = float(gamma_t)
        self.agent.gamma = self.gamma_t

        # Infer policy posterior
        q_pi, efe = self.agent.infer_policies()

        # Sample action
        chosen_action_ids = self.agent.sample_action()
        u_choice = int(chosen_action_ids[1]) #We choose id 1 because 0 is just "rest" (context action, which we can't change)
        action_label = self.action_id_to_label(chosen_action_ids)

        # COMPUTE LOG-LIKELIHOOD OF THE CHOSEN ACTION
        # Find which policies lead to this action
        action_ll = -np.inf
        for pi_idx, policy in enumerate(self.agent.policies):
            # Check if first action of this policy matches chosen action
            if int(policy[0, 1]) == u_choice:  # factor 1, timestep 0
                # Add this policy's contribution to action probability
                if action_ll == -np.inf:
                    action_ll = np.log(q_pi[pi_idx] + 1e-16)
                else:
                    # Log-sum-exp trick for numerical stability
                    action_ll = np.logaddexp(action_ll, np.log(q_pi[pi_idx] + 1e-16))

        return action_label, qs, q_pi, efe, self.gamma_t, action_ll


def run_episode_with_ll(runner, env, T=200, verbose=False, initial_obs_labels=None):
    """
    Run episode and track per-trial log-likelihoods.

    Returns:
    --------
    logs : dict
        Episode logs including 'll' field with per-trial log-likelihoods
    """
    if initial_obs_labels is None:
        initial_obs_labels = ['null', 'null', 'observe_start']

    obs_ids = runner.obs_labels_to_ids(initial_obs_labels)

    logs = {
        't': [],
        'context': [],
        'belief': [],
        'gamma': [],
        'action': [],
        'reward_label': [],
        'choice_label': [],
        'll': []  # Per-trial log-likelihood
    }

    for t in range(T):
        # Agent step with LL tracking
        if hasattr(runner, 'step_with_ll'):
            action_label, qs, q_pi, efe, gamma_t, ll_t = runner.step_with_ll(obs_ids, t)
        else:
            # Fallback if using regular AgentRunner
            action_label, qs, q_pi, efe, gamma_t = runner.step(obs_ids, t)
            ll_t = 0.0

        # Environment step
        obs_labels = env.step(action_label)
        obs_ids = runner.obs_labels_to_ids(obs_labels)

        # Logging
        logs['t'].append(t)
        logs['context'].append(env.context)
        logs['belief'].append(qs[0].copy())
        logs['gamma'].append(gamma_t)
        logs['action'].append(action_label)
        logs['reward_label'].append(obs_labels[1])
        logs['choice_label'].append(obs_labels[2])
        logs['ll'].append(ll_t)

    return logs


# ============================================================================
# MAIN COMPARISON FUNCTION
# ============================================================================

def compare_models(n_seeds=20, trials=200, reversal_schedule=None, verbose=True):
    """
    Run rigorous comparison of M1, M2, and M3 across multiple seeds.
    """

    if reversal_schedule is None:
        reversal_schedule = [30, 60, 90, 120, 150, 180]

    # Storage for results
    results = {
        'seeds': [],
        'M1': {'accuracy': [], 'll': [], 'gamma': [], 'entropy': []},
        'M2': {'accuracy': [], 'll': [], 'gamma': [], 'entropy': []},
        'M3': {'accuracy': [], 'll': [], 'gamma': [], 'entropy': []}
    }

    iterator = tqdm(range(n_seeds), desc="Running seeds") if verbose else range(n_seeds)

    for seed in iterator:
        results['seeds'].append(seed)

        # Test each model with the same seed
        for model_name in ['M1', 'M2', 'M3']:
            np.random.seed(seed)

            # Build environment
            env = TwoArmedBandit(
                probability_hint=probability_hint,
                probability_reward=probability_reward,
                reversal_schedule=reversal_schedule.copy()
            )

            # Build generative model
            A = build_A(num_modalities, state_contexts, state_choices,
                       observation_hints, observation_rewards, observation_choices,
                       probability_hint, probability_reward)
            B = build_B(state_contexts, state_choices, action_contexts,
                       action_choices, context_volatility=0.05)
            D = build_D(state_contexts, state_choices)

            # Create model-specific value function
            if model_name == 'M1':
                value_fn = make_value_fn('M1', C_reward_logits=[0.0, -4.0, 2.0], gamma=1.2)

            elif model_name == 'M2':
                value_fn = make_value_fn('M2', C_reward_logits=[0.0, -4.0, 2.0], gamma_schedule=lambda q,t: gamma_entropy_coupled(q, t, g_base=1.6, k=1.0))

            elif model_name == 'M3':
                # Get policies for M3
                A_temp = build_A(num_modalities, state_contexts, state_choices,
                               observation_hints, observation_rewards, observation_choices,
                               probability_hint, probability_reward)
                B_temp = build_B(state_contexts, state_choices, action_contexts, action_choices)
                D_temp = build_D(state_contexts, state_choices)
                C_temp = utils.obj_array_zeros([(A_temp[m].shape[0],) for m in range(len(A_temp))])

                temp_agent = Agent(A=A_temp, B=B_temp, C=C_temp, D=D_temp,
                                 policy_len=2, inference_horizon=1,
                                 control_fac_idx=[1], use_utility=True,
                                 use_states_info_gain=True,
                                 action_selection="stochastic", gamma=16)

                policies = temp_agent.policies
                num_actions_per_factor = [len(action_contexts), len(action_choices)]

                profiles = [
                    {'phi_logits': [0.0, -2.0, 1.5], 'xi_logits': [0.0, 0.5, 0.0, 0.0], 'gamma': 0.6},
                    {'phi_logits': [0.0, -8.0, 4.0], 'xi_logits': [0.0, -1.0, 0.0, 0.0], 'gamma': 3.0},
                ]

                Z = np.array([[0.8, 0.2],
                              [0.2, 0.8]])

                value_fn = make_value_fn('M3', profiles=profiles, Z=Z,
                                        policies=policies,
                                        num_actions_per_factor=num_actions_per_factor)

            # Create agent runner with LL tracking
            runner = AgentRunnerWithLL(A, B, D, value_fn,
                                       observation_hints, observation_rewards,
                                       observation_choices, action_choices,
                                       reward_mod_idx=1)

            # Run episode
            logs = run_episode_with_ll(runner, env, T=trials, verbose=False)

            # Compute metrics
            acc = trial_accuracy(logs['action'], logs['context'])
            mean_acc = acc.mean()

            # Sum log-likelihoods across trials
            total_ll = np.sum(logs['ll'])

            # Compute entropy
            entropies = [compute_entropy(b) for b in logs['belief']]
            mean_entropy = np.mean(entropies)

            # Store results
            results[model_name]['accuracy'].append(mean_acc)
            results[model_name]['ll'].append(total_ll)
            results[model_name]['gamma'].append(np.mean(logs['gamma']))
            results[model_name]['entropy'].append(mean_entropy)

    return results


# ============================================================================
# STATISTICAL ANALYSIS FUNCTIONS
# ============================================================================

def bootstrap_ci(data, n_bootstrap=10000, ci=95):
    """Compute bootstrap confidence interval."""
    data = np.array(data)
    means = []

    for _ in range(n_bootstrap):
        sample = np.random.choice(data, size=len(data), replace=True)
        means.append(np.mean(sample))

    mean = np.mean(data)
    alpha = (100 - ci) / 2
    ci_lower = np.percentile(means, alpha)
    ci_upper = np.percentile(means, 100 - alpha)

    return mean, ci_lower, ci_upper


def analyze_results(results, metric='ll'):
    """Perform statistical analysis and print results."""

    print(f"\n{'='*70}")
    print(f"STATISTICAL ANALYSIS: {metric.upper()}")
    print(f"{'='*70}\n")

    m1 = np.array(results['M1'][metric])
    m2 = np.array(results['M2'][metric])
    m3 = np.array(results['M3'][metric])

    analysis = {}

    # Check for valid data
    if np.all(m1 == 0) and np.all(m2 == 0) and np.all(m3 == 0):
        print(f"WARNING: All {metric} values are zero. Log-likelihood may not be computed correctly.")
        print("This usually means the LL tracking failed.\n")

    # Individual model statistics
    for model_name, data in [('M1', m1), ('M2', m2), ('M3', m3)]:
        mean, ci_low, ci_high = bootstrap_ci(data)
        print(f"{model_name}:")
        print(f"  Mean {metric}: {mean:.4f}")
        print(f"  95% CI: [{ci_low:.4f}, {ci_high:.4f}]")
        print(f"  Std: {np.std(data):.4f}")
        print()

        analysis[model_name] = {
            'mean': mean,
            'ci_lower': ci_low,
            'ci_upper': ci_high,
            'std': np.std(data)
        }

    # Pairwise comparisons
    print(f"\n{'-'*70}")
    print("PAIRWISE COMPARISONS")
    print(f"{'-'*70}\n")

    comparisons = [('M3', 'M1'), ('M3', 'M2'), ('M2', 'M1')]

    for model_a, model_b in comparisons:
        data_a = np.array(results[model_a][metric])
        data_b = np.array(results[model_b][metric])

        delta = data_a - data_b
        mean_delta, ci_low, ci_high = bootstrap_ci(delta)

        # Handle case where there's no variance
        if np.std(delta) > 1e-10:
            t_stat, p_value = stats.ttest_rel(data_a, data_b)
            d = mean_delta / np.std(delta)
        else:
            p_value = 1.0
            d = 0.0

        print(f"{model_a} vs {model_b}:")
        print(f"  Δ{metric} = {mean_delta:.4f}")
        print(f"  95% CI: [{ci_low:.4f}, {ci_high:.4f}]")
        print(f"  p-value: {p_value:.4e}")
        print(f"  Cohen's d: {d:.3f}")

        if ci_low > 0:
            print(f"  ✓ {model_a} significantly BETTER (95% CI excludes 0)")
        elif ci_high < 0:
            print(f"  ✗ {model_a} significantly WORSE (95% CI excludes 0)")
        else:
            print(f"  ~ No significant difference (95% CI includes 0)")
        print()

        analysis[f'{model_a}_vs_{model_b}'] = {
            'mean_delta': mean_delta,
            'ci_lower': ci_low,
            'ci_upper': ci_high,
            'p_value': p_value,
            'cohens_d': d,
            'significant': ci_low > 0 or ci_high < 0
        }

    return analysis


# ============================================================================
# VISUALIZATION FUNCTIONS
# ============================================================================

def plot_comparison_results(results, save_path=None):
    """Create comprehensive visualization of model comparison results."""

    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    fig.suptitle('Model Comparison Results', fontsize=16, fontweight='bold')

    metrics = ['accuracy', 'll', 'gamma', 'entropy']
    metric_labels = ['Accuracy', 'Log-Likelihood', 'Mean γ', 'Mean Entropy']

    for idx, (metric, label) in enumerate(zip(metrics, metric_labels)):
        if idx < 4:
            row = idx // 2
            col = idx % 2
            ax = axes[row, col]
        else:
            continue

        data = [results['M1'][metric], results['M2'][metric], results['M3'][metric]]

        bp = ax.boxplot(data, tick_labels=['M1', 'M2', 'M3'], patch_artist=True)

        colors = ['lightblue', 'lightgreen', 'lightcoral']
        for patch, color in zip(bp['boxes'], colors):
            patch.set_facecolor(color)

        for i, d in enumerate(data):
            x = np.random.normal(i+1, 0.04, size=len(d))
            ax.scatter(x, d, alpha=0.3, s=20, color='black')

        ax.set_ylabel(label)
        ax.set_xlabel('Model')
        ax.grid(True, alpha=0.3)
        ax.set_title(f'{label} Distribution')

    # Pairwise difference plots
    ax = axes[1, 2]

    delta_m3_m1 = np.array(results['M3']['ll']) - np.array(results['M1']['ll'])
    delta_m3_m2 = np.array(results['M3']['ll']) - np.array(results['M2']['ll'])

    ax.axhline(0, color='black', linestyle='--', alpha=0.5)
    ax.boxplot([delta_m3_m1, delta_m3_m2], tick_labels=['M3-M1', 'M3-M2'])
    ax.set_ylabel('ΔLog-Likelihood')
    ax.set_title('Log-Likelihood Improvement')
    ax.grid(True, alpha=0.3)

    for i, delta in enumerate([delta_m3_m1, delta_m3_m2]):
        ax.hlines(np.mean(delta), i+0.7, i+1.3, colors='red', linewidth=2)

    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"\nFigure saved to: {save_path}")

    plt.show()


# ============================================================================
# MAIN EXECUTION
# ============================================================================

def run_full_comparison(n_seeds=20, trials=200, reversal_schedule=None):
    """Run complete model comparison pipeline."""

    print("="*70)
    print("MODEL COMPARISON TEST SUITE")
    print("="*70)
    print(f"\nConfiguration:")
    print(f"  - Number of seeds: {n_seeds}")
    print(f"  - Trials per episode: {trials}")
    print(f"  - Reversal schedule: {reversal_schedule}")
    print(f"\nRunning simulations...\n")

    results = compare_models(
        n_seeds=n_seeds,
        trials=trials,
        reversal_schedule=reversal_schedule,
        verbose=True
    )

    print("\n" + "="*70)
    print("PRIMARY METRIC: LOG-LIKELIHOOD")
    print("="*70)
    analysis_ll = analyze_results(results, metric='ll')

    print("\n" + "="*70)
    print("SECONDARY METRIC: ACCURACY")
    print("="*70)
    analysis_acc = analyze_results(results, metric='accuracy')

    print("\n" + "="*70)
    print("GENERATING VISUALIZATIONS")
    print("="*70)
    plot_comparison_results(results)

    print("\n" + "="*70)
    print("FINAL VERDICT")
    print("="*70)

    ll_m3_vs_m1 = analysis_ll['M3_vs_M1']
    ll_m3_vs_m2 = analysis_ll['M3_vs_M2']

    if ll_m3_vs_m1['significant'] and ll_m3_vs_m1['ci_lower'] > 0:
        print("\n✓ M3 SIGNIFICANTLY OUTPERFORMS M1 on log-likelihood")
        print(f"  Mean improvement: {ll_m3_vs_m1['mean_delta']:.4f}")
        print(f"  95% CI: [{ll_m3_vs_m1['ci_lower']:.4f}, {ll_m3_vs_m1['ci_upper']:.4f}]")
    else:
        print("\n✗ M3 does NOT significantly outperform M1")

    if ll_m3_vs_m2['significant'] and ll_m3_vs_m2['ci_lower'] > 0:
        print("\n✓ M3 SIGNIFICANTLY OUTPERFORMS M2 on log-likelihood")
        print(f"  Mean improvement: {ll_m3_vs_m2['mean_delta']:.4f}")
        print(f"  95% CI: [{ll_m3_vs_m2['ci_lower']:.4f}, {ll_m3_vs_m2['ci_upper']:.4f}]")
    else:
        print("\n✗ M3 does NOT significantly outperform M2")

    return results, analysis_ll, analysis_acc

"""### Run Model Comparison Test - Basic"""

results, analysis_ll, analysis_acc = run_full_comparison(n_seeds=20, trials=200, reversal_schedule=[30, 60, 90, 120, 150, 180])

"""### Model Comparison across different reversal schedules"""

for sched in [[20,40,60,80,100,120], [50,100,150], [25,75,125,175]]:
    results, ll, acc = run_full_comparison(n_seeds=20, trials=200, reversal_schedule=sched)

"""### AIC / BIC Model Comparison"""

"""
AIC/BIC Model Comparison Cell
"""

# ============================================================================
# AIC/BIC COMPUTATION FUNCTIONS
# ============================================================================

def count_model_parameters(model_name, num_obs, num_states, num_actions, profiles=None):
    """
    Count the number of free parameters in each model.

    Parameters:
    -----------
    model_name : str
        'M1', 'M2', or 'M3'
    num_obs : list
        Number of observations per modality [3, 3, 4]
    num_states : list
        Number of states per factor [2, 4]
    num_actions : list
        Number of actions per factor [1, 4]
    profiles : list (for M3)
        Profile definitions

    Returns:
    --------
    k : int
        Number of free parameters
    """

    if model_name == 'M1':
        # M1: Static global model
        # C vector: reward modality (3 outcomes, but sum-to-1 constraint) = 2 free params
        # gamma: 1 scalar parameter
        k = 2 + 1  # C_reward (2) + gamma (1)

    elif model_name == 'M2':
        # M2: Dynamic global precision
        # C vector: 2 free params (same as M1)
        # gamma_schedule: 2 params (g_base, k)
        k = 2 + 2  # C_reward (2) + gamma_params (2)

    elif model_name == 'M3':
        # M3: Profile model
        # Per profile:
        #   - phi_logits: 2 free params (3 outcomes - 1 constraint)
        #   - xi_logits: 3 free params (4 actions - 1 constraint)
        #   - gamma: 1 scalar
        # Total per profile: 6 params
        # Z matrix: (2 states × 2 profiles) - 2 constraints (rows sum to 1) = 2 free params

        num_profiles = len(profiles) if profiles else 2
        params_per_profile = 2 + 3 + 1  # phi (2) + xi (3) + gamma (1)
        params_Z = (num_states[0] - 1) * num_profiles  # Z matrix free params

        k = num_profiles * params_per_profile + params_Z
        # With 2 profiles: 2*6 + 2 = 14 params

    else:
        raise ValueError(f"Unknown model: {model_name}")

    return k


def compute_aic_bic(log_likelihood, n_params, n_trials):
    """
    Compute AIC and BIC given log-likelihood, parameters, and sample size.

    AIC = 2k - 2*LL
    BIC = k*ln(n) - 2*LL

    where:
    - k = number of parameters
    - LL = log-likelihood
    - n = number of observations (trials)

    Lower values indicate better models.
    """
    aic = 2 * n_params - 2 * log_likelihood
    bic = n_params * np.log(n_trials) - 2 * log_likelihood

    return aic, bic


def run_aic_bic_comparison(n_seeds=20, trials=200, reversal_schedule=None, verbose=True):
    """
    Run model comparison using AIC and BIC.
    """

    if reversal_schedule is None:
        reversal_schedule = [30, 60, 90, 120, 150, 180]

    # Define model parameters
    model_params = {
        'M1': count_model_parameters('M1', num_obs, num_states, num_actions),
        'M2': count_model_parameters('M2', num_obs, num_states, num_actions),
        'M3': count_model_parameters('M3', num_obs, num_states, num_actions,
                                     profiles=[{}, {}])  # 2 profiles
    }

    print("="*70)
    print("MODEL PARAMETER COUNTS")
    print("="*70)
    for model, k in model_params.items():
        print(f"{model}: {k} parameters")
    print()

    # Storage for results
    results = {
        'seeds': [],
        'M1': {'ll': [], 'aic': [], 'bic': []},
        'M2': {'ll': [], 'aic': [], 'bic': []},
        'M3': {'ll': [], 'aic': [], 'bic': []}
    }

    iterator = tqdm(range(n_seeds), desc="Running AIC/BIC comparison") if verbose else range(n_seeds)

    for seed in iterator:
        results['seeds'].append(seed)

        for model_name in ['M1', 'M2', 'M3']:
            np.random.seed(seed)

            # Build environment
            env = TwoArmedBandit(
                probability_hint=probability_hint,
                probability_reward=probability_reward,
                reversal_schedule=reversal_schedule.copy()
            )

            # Build generative model
            A = build_A(num_modalities, state_contexts, state_choices,
                       observation_hints, observation_rewards, observation_choices,
                       probability_hint, probability_reward)
            B = build_B(state_contexts, state_choices, action_contexts,
                       action_choices, context_volatility=0.05)
            D = build_D(state_contexts, state_choices)

            # Create model-specific value function
            if model_name == 'M1':
                value_fn = make_value_fn('M1', C_reward_logits=[0.0, -4.0, 2.0], gamma=1.2)

            elif model_name == 'M2':
                value_fn = make_value_fn('M2', C_reward_logits=[0.0, -4.0, 2.0],
                                        gamma_schedule=lambda q,t: gamma_entropy_coupled(q, t, g_base=1.6, k=1.0))

            elif model_name == 'M3':
                # Get policies
                A_temp = build_A(num_modalities, state_contexts, state_choices,
                               observation_hints, observation_rewards, observation_choices,
                               probability_hint, probability_reward)
                B_temp = build_B(state_contexts, state_choices, action_contexts, action_choices)
                D_temp = build_D(state_contexts, state_choices)
                C_temp = utils.obj_array_zeros([(A_temp[m].shape[0],) for m in range(len(A_temp))])

                temp_agent = Agent(A=A_temp, B=B_temp, C=C_temp, D=D_temp,
                                 policy_len=2, inference_horizon=1,
                                 control_fac_idx=[1], use_utility=True,
                                 use_states_info_gain=True,
                                 action_selection="stochastic", gamma=16)

                policies = temp_agent.policies
                num_actions_per_factor = [len(action_contexts), len(action_choices)]

                profiles = [
                    {'phi_logits': [0.0, -2.0, 1.5], 'xi_logits': [0.0, 0.5, 0.0, 0.0], 'gamma': 0.6},
                    {'phi_logits': [0.0, -8.0, 4.0], 'xi_logits': [0.0, -1.0, 0.0, 0.0], 'gamma': 3.0},
                ]

                Z = np.array([[0.8, 0.2], [0.2, 0.8]])

                value_fn = make_value_fn('M3', profiles=profiles, Z=Z,
                                        policies=policies,
                                        num_actions_per_factor=num_actions_per_factor)

            # Create agent runner with LL tracking
            runner = AgentRunnerWithLL(A, B, D, value_fn,
                                       observation_hints, observation_rewards,
                                       observation_choices, action_choices,
                                       reward_mod_idx=1)

            # Run episode
            logs = run_episode_with_ll(runner, env, T=trials, verbose=False)

            # Compute log-likelihood
            total_ll = np.sum(logs['ll'])

            # Compute AIC and BIC
            k = model_params[model_name]
            aic, bic = compute_aic_bic(total_ll, k, trials)

            # Store results
            results[model_name]['ll'].append(total_ll)
            results[model_name]['aic'].append(aic)
            results[model_name]['bic'].append(bic)

    return results, model_params


def analyze_aic_bic(results, model_params):
    """Analyze and display AIC/BIC results."""

    print("\n" + "="*70)
    print("AIC/BIC RESULTS")
    print()

    # Compute means and CIs
    for metric in ['aic', 'bic']:
        print(f"\n{'-'*70}")
        print(f"{metric.upper()} COMPARISON")
        print(f"{'-'*70}\n")

        metric_results = {}

        for model in ['M1', 'M2', 'M3']:
            data = np.array(results[model][metric])
            mean, ci_low, ci_high = bootstrap_ci(data)

            metric_results[model] = {
                'mean': mean,
                'ci_low': ci_low,
                'ci_high': ci_high,
                'std': np.std(data)
            }

            print(f"{model} (k={model_params[model]}):")
            print(f"  Mean {metric.upper()}: {mean:.2f}")
            print(f"  95% CI: [{ci_low:.2f}, {ci_high:.2f}]")
            print(f"  Std: {np.std(data):.2f}")
            print()

        # Find best model
        best_model = min(metric_results.items(), key=lambda x: x[1]['mean'])[0]
        print(f"Best model by {metric.upper()}: {best_model}")

        # Compute differences from best
        print(f"{metric.upper()} relative to {best_model}:")
        for model in ['M1', 'M2', 'M3']:
            if model != best_model:
                delta = np.array(results[model][metric]) - np.array(results[best_model][metric])
                mean_delta, ci_low, ci_high = bootstrap_ci(delta)

                print(f"  {model}: +{mean_delta:.2f} (95% CI: [{ci_low:.2f}, {ci_high:.2f}])")
        print()


def plot_aic_bic_comparison(results, model_params, save_path=None):
    """Visualize AIC/BIC comparison."""

    fig, axes = plt.subplots(1, 3, figsize=(15, 5))
    fig.suptitle('Model Comparison: AIC/BIC', fontsize=16, fontweight='bold')

    # Plot 1: Log-Likelihood
    ax = axes[0]
    data = [results['M1']['ll'], results['M2']['ll'], results['M3']['ll']]
    bp = ax.boxplot(data, tick_labels=['M1', 'M2', 'M3'], patch_artist=True)

    colors = ['lightblue', 'lightgreen', 'lightcoral']
    for patch, color in zip(bp['boxes'], colors):
        patch.set_facecolor(color)

    ax.set_ylabel('Log-Likelihood')
    ax.set_xlabel('Model')
    ax.set_title('Log-Likelihood')
    ax.grid(True, alpha=0.3)

    # Plot 2: AIC
    ax = axes[1]
    data = [results['M1']['aic'], results['M2']['aic'], results['M3']['aic']]
    bp = ax.boxplot(data, tick_labels=['M1', 'M2', 'M3'], patch_artist=True)

    for patch, color in zip(bp['boxes'], colors):
        patch.set_facecolor(color)

    # Add parameter counts as text
    for i, model in enumerate(['M1', 'M2', 'M3']):
        ax.text(i+1, ax.get_ylim()[1]*0.95, f'k={model_params[model]}',
                ha='center', fontsize=9, bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

    ax.set_ylabel('AIC')
    ax.set_xlabel('Model')
    ax.set_title('AIC')
    ax.grid(True, alpha=0.3)

    # Plot 3: BIC
    ax = axes[2]
    data = [results['M1']['bic'], results['M2']['bic'], results['M3']['bic']]
    bp = ax.boxplot(data, tick_labels=['M1', 'M2', 'M3'], patch_artist=True)

    for patch, color in zip(bp['boxes'], colors):
        patch.set_facecolor(color)

    # Add parameter counts as text
    for i, model in enumerate(['M1', 'M2', 'M3']):
        ax.text(i+1, ax.get_ylim()[1]*0.95, f'k={model_params[model]}',
                ha='center', fontsize=9, bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

    ax.set_ylabel('BIC')
    ax.set_xlabel('Model')
    ax.set_title('BIC')
    ax.grid(True, alpha=0.3)

    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"\nFigure saved to: {save_path}")

    plt.show()


# ============================================================================
# MAIN EXECUTION
# ============================================================================

print("="*70)
print("AIC/BIC MODEL COMPARISON")
print("="*70)
print("\nRunning comparison with information criteria...")
print("This accounts for model complexity (number of parameters)")
print()

# Run comparison
results_aic_bic, model_params = run_aic_bic_comparison(
    n_seeds=20,
    trials=200,
    reversal_schedule=[30, 60, 90, 120, 150, 180],
    verbose=True
)

# Analyze results
analyze_aic_bic(results_aic_bic, model_params)

# Plot results
plot_aic_bic_comparison(results_aic_bic, model_params)

"""
Robustness Across Reward Noise Test
Tests whether M3's advantage grows as environmental uncertainty increases
"""

# ============================================================================
# NOISE ROBUSTNESS TEST
# ============================================================================

def run_noise_robustness_test(
    n_seeds=15,
    trials=200,
    reversal_schedule=None,
    noise_levels=None,
    verbose=True
):
    """
    Test model performance across different levels of environmental noise.

    Hypothesis: M3's advantage should GROW as noise increases because it can
    adaptively adjust precision based on uncertainty.

    Parameters:
    -----------
    n_seeds : int
        Number of random seeds per noise level
    trials : int
        Number of trials per episode
    reversal_schedule : list
        Trial indices where context reverses
    noise_levels : list of dict
        Each dict contains 'p_reward' and 'p_hint' values to test
    verbose : bool
        Show progress bars

    Returns:
    --------
    results : dict
        Performance metrics across noise levels
    """

    if reversal_schedule is None:
        reversal_schedule = [30, 60, 90, 120, 150, 180]

    if noise_levels is None:
        # Define noise levels from low to high uncertainty
        noise_levels = [
            {'name': 'Low Noise', 'p_reward': 0.95, 'p_hint': 0.95},
            {'name': 'Medium-Low', 'p_reward': 0.85, 'p_hint': 0.80},
            {'name': 'Medium', 'p_reward': 0.75, 'p_hint': 0.70},
            {'name': 'Medium-High', 'p_reward': 0.65, 'p_hint': 0.65},
            {'name': 'High Noise', 'p_reward': 0.55, 'p_hint': 0.60},
        ]

    print("="*70)
    print("ROBUSTNESS ACROSS REWARD NOISE TEST")
    print("="*70)
    print(f"\nTesting {len(noise_levels)} noise levels × 3 models × {n_seeds} seeds")
    print(f"= {len(noise_levels) * 3 * n_seeds} total simulations\n")

    # Display noise levels
    print("Noise Levels:")
    for nl in noise_levels:
        print(f"  {nl['name']:15s}: p_reward={nl['p_reward']:.2f}, p_hint={nl['p_hint']:.2f}")
    print()

    # Storage for results
    results = {
        'noise_levels': noise_levels,
        'M1': {nl['name']: {'ll': [], 'accuracy': [], 'gamma': [], 'entropy': []}
               for nl in noise_levels},
        'M2': {nl['name']: {'ll': [], 'accuracy': [], 'gamma': [], 'entropy': []}
               for nl in noise_levels},
        'M3': {nl['name']: {'ll': [], 'accuracy': [], 'gamma': [], 'entropy': []}
               for nl in noise_levels},
    }

    # Run simulations
    total_sims = len(noise_levels) * 3 * n_seeds
    pbar = tqdm(total=total_sims, desc="Running simulations") if verbose else None

    for noise_config in noise_levels:
        noise_name = noise_config['name']
        p_reward_env = noise_config['p_reward']
        p_hint_env = noise_config['p_hint']

        for seed in range(n_seeds):
            for model_name in ['M1', 'M2', 'M3']:
                np.random.seed(seed)

                # Build environment with current noise level
                env = TwoArmedBandit(
                    probability_hint=p_hint_env,
                    probability_reward=p_reward_env,
                    reversal_schedule=reversal_schedule.copy()
                )

                # Build generative model (agent's beliefs - keep fixed)
                # Agent believes standard probabilities (0.7 hint, 0.8 reward)
                A = build_A(num_modalities, state_contexts, state_choices,
                           observation_hints, observation_rewards, observation_choices,
                           0.7, 0.8)  # Agent's beliefs stay constant
                B = build_B(state_contexts, state_choices, action_contexts,
                           action_choices, context_volatility=0.05)
                D = build_D(state_contexts, state_choices)

                # Create model-specific value function
                if model_name == 'M1':
                    value_fn = make_value_fn('M1', C_reward_logits=[0.0, -4.0, 2.0], gamma=1.2)

                elif model_name == 'M2':
                    value_fn = make_value_fn('M2', C_reward_logits=[0.0, -4.0, 2.0],
                                            gamma_schedule=lambda q,t: gamma_entropy_coupled(q, t, g_base=1.6, k=1.0))

                elif model_name == 'M3':
                    # Get policies
                    A_temp = build_A(num_modalities, state_contexts, state_choices,
                                   observation_hints, observation_rewards, observation_choices,
                                   0.7, 0.8)
                    B_temp = build_B(state_contexts, state_choices, action_contexts, action_choices)
                    D_temp = build_D(state_contexts, state_choices)
                    C_temp = utils.obj_array_zeros([(A_temp[m].shape[0],) for m in range(len(A_temp))])

                    temp_agent = Agent(A=A_temp, B=B_temp, C=C_temp, D=D_temp,
                                     policy_len=2, inference_horizon=1,
                                     control_fac_idx=[1], use_utility=True,
                                     use_states_info_gain=True,
                                     action_selection="stochastic", gamma=16)

                    policies = temp_agent.policies
                    num_actions_per_factor = [len(action_contexts), len(action_choices)]

                    profiles = [
                        {'phi_logits': [0.0, -2.0, 1.5], 'xi_logits': [0.0, 0.5, 0.0, 0.0], 'gamma': 0.6},
                        {'phi_logits': [0.0, -8.0, 4.0], 'xi_logits': [0.0, -1.0, 0.0, 0.0], 'gamma': 3.0},
                    ]

                    Z = np.array([[0.8, 0.2], [0.2, 0.8]])

                    value_fn = make_value_fn('M3', profiles=profiles, Z=Z,
                                            policies=policies,
                                            num_actions_per_factor=num_actions_per_factor)

                # Create agent runner with LL tracking
                runner = AgentRunnerWithLL(A, B, D, value_fn,
                                           observation_hints, observation_rewards,
                                           observation_choices, action_choices,
                                           reward_mod_idx=1)

                # Run episode
                logs = run_episode_with_ll(runner, env, T=trials, verbose=False)

                # Compute metrics
                acc = trial_accuracy(logs['action'], logs['context'])
                total_ll = np.sum(logs['ll'])
                entropies = [compute_entropy(b) for b in logs['belief']]
                mean_entropy = np.mean(entropies)
                mean_gamma = np.mean(logs['gamma'])

                # Store results
                results[model_name][noise_name]['ll'].append(total_ll)
                results[model_name][noise_name]['accuracy'].append(acc.mean())
                results[model_name][noise_name]['gamma'].append(mean_gamma)
                results[model_name][noise_name]['entropy'].append(mean_entropy)

                if pbar:
                    pbar.update(1)

    if pbar:
        pbar.close()

    return results


def analyze_noise_robustness(results):
    """
    Analyze how model performance changes with noise.
    Focus on whether M3's advantage GROWS with uncertainty.
    """

    noise_levels = results['noise_levels']

    print("\n" + "="*70)
    print("NOISE ROBUSTNESS ANALYSIS")
    print("="*70)

    # For each noise level, compute mean performance
    print("\n" + "-"*70)
    print("ACCURACY BY NOISE LEVEL")
    print("-"*70)
    print(f"{'Noise Level':<15} {'M1':>10} {'M2':>10} {'M3':>10} {'M3-M1':>10} {'M3-M2':>10}")
    print("-"*70)

    acc_summary = []

    for nl in noise_levels:
        name = nl['name']

        m1_acc = np.mean(results['M1'][name]['accuracy'])
        m2_acc = np.mean(results['M2'][name]['accuracy'])
        m3_acc = np.mean(results['M3'][name]['accuracy'])

        diff_m3_m1 = m3_acc - m1_acc
        diff_m3_m2 = m3_acc - m2_acc

        print(f"{name:<15} {m1_acc:>10.3f} {m2_acc:>10.3f} {m3_acc:>10.3f} "
              f"{diff_m3_m1:>10.3f} {diff_m3_m2:>10.3f}")

        acc_summary.append({
            'noise': name,
            'p_reward': nl['p_reward'],
            'm1': m1_acc,
            'm2': m2_acc,
            'm3': m3_acc,
            'diff_m3_m1': diff_m3_m1,
            'diff_m3_m2': diff_m3_m2
        })

    # Check if M3's advantage grows with noise
    print("\n" + "-"*70)
    print("LOG-LIKELIHOOD BY NOISE LEVEL")
    print("-"*70)
    print(f"{'Noise Level':<15} {'M1':>10} {'M2':>10} {'M3':>10} {'M3-M1':>10} {'M3-M2':>10}")
    print("-"*70)

    ll_summary = []

    for nl in noise_levels:
        name = nl['name']

        m1_ll = np.mean(results['M1'][name]['ll'])
        m2_ll = np.mean(results['M2'][name]['ll'])
        m3_ll = np.mean(results['M3'][name]['ll'])

        diff_m3_m1 = m3_ll - m1_ll
        diff_m3_m2 = m3_ll - m2_ll

        print(f"{name:<15} {m1_ll:>10.2f} {m2_ll:>10.2f} {m3_ll:>10.2f} "
              f"{diff_m3_m1:>10.2f} {diff_m3_m2:>10.2f}")

        ll_summary.append({
            'noise': name,
            'p_reward': nl['p_reward'],
            'm1': m1_ll,
            'm2': m2_ll,
            'm3': m3_ll,
            'diff_m3_m1': diff_m3_m1,
            'diff_m3_m2': diff_m3_m2
        })

    # Test for trend: Does M3's advantage correlate with noise?
    print("\n" + "="*70)
    print("TREND ANALYSIS: M3 ADVANTAGE vs NOISE")
    print("="*70)

    # Create uncertainty metric (inverse of reliability)
    uncertainty = [1.0 - nl['p_reward'] for nl in noise_levels]

    # Test correlation between uncertainty and M3 advantage
    from scipy.stats import pearsonr, spearmanr

    acc_advantage = [s['diff_m3_m1'] for s in acc_summary]
    ll_advantage = [s['diff_m3_m1'] for s in ll_summary]

    r_acc, p_acc = pearsonr(uncertainty, acc_advantage)
    r_ll, p_ll = pearsonr(uncertainty, ll_advantage)

    rho_acc, p_rho_acc = spearmanr(uncertainty, acc_advantage)
    rho_ll, p_rho_ll = spearmanr(uncertainty, ll_advantage)

    print(f"\nAccuracy advantage (M3-M1) vs Uncertainty:")
    print(f"  Pearson r = {r_acc:.3f}, p = {p_acc:.4f}")
    print(f"  Spearman ρ = {rho_acc:.3f}, p = {p_rho_acc:.4f}")

    if p_acc < 0.05:
        if r_acc > 0:
            print(f"  ✓ M3's advantage GROWS with uncertainty (positive correlation)")
        else:
            print(f"  ✗ M3's advantage SHRINKS with uncertainty (negative correlation)")
    else:
        print(f"  ~ No significant correlation")

    print(f"\nLog-likelihood advantage (M3-M1) vs Uncertainty:")
    print(f"  Pearson r = {r_ll:.3f}, p = {p_ll:.4f}")
    print(f"  Spearman ρ = {rho_ll:.3f}, p = {p_rho_ll:.4f}")

    if p_ll < 0.05:
        if r_ll > 0:
            print(f"  ✓ M3's advantage GROWS with uncertainty (positive correlation)")
        else:
            print(f"  ✗ M3's advantage SHRINKS with uncertainty (negative correlation)")
    else:
        print(f"  ~ No significant correlation")

    return acc_summary, ll_summary


def plot_noise_robustness(results, acc_summary, ll_summary, save_path=None):
    """
    Visualize how model performance changes with environmental noise.
    """

    noise_levels = results['noise_levels']
    uncertainty = [1.0 - nl['p_reward'] for nl in noise_levels]
    noise_names = [nl['name'] for nl in noise_levels]

    fig, axes = plt.subplots(2, 3, figsize=(16, 10))
    fig.suptitle('Model Robustness Across Environmental Noise', fontsize=16, fontweight='bold')

    # Plot 1: Accuracy vs Noise
    ax = axes[0, 0]
    for model in ['M1', 'M2', 'M3']:
        accs = [np.mean(results[model][nl['name']]['accuracy']) for nl in noise_levels]
        ax.plot(uncertainty, accs, marker='o', linewidth=2, label=model, markersize=8)

    ax.set_xlabel('Environmental Uncertainty (1 - p_reward)')
    ax.set_ylabel('Mean Accuracy')
    ax.set_title('Accuracy vs Noise')
    ax.legend()
    ax.grid(True, alpha=0.3)

    # Plot 2: Log-Likelihood vs Noise
    ax = axes[0, 1]
    for model in ['M1', 'M2', 'M3']:
        lls = [np.mean(results[model][nl['name']]['ll']) for nl in noise_levels]
        ax.plot(uncertainty, lls, marker='o', linewidth=2, label=model, markersize=8)

    ax.set_xlabel('Environmental Uncertainty (1 - p_reward)')
    ax.set_ylabel('Mean Log-Likelihood')
    ax.set_title('Log-Likelihood vs Noise')
    ax.legend()
    ax.grid(True, alpha=0.3)

    # Plot 3: M3 Advantage vs Noise (Accuracy)
    ax = axes[0, 2]
    acc_adv = [s['diff_m3_m1'] for s in acc_summary]
    ax.plot(uncertainty, acc_adv, marker='o', linewidth=2, color='green', label='M3-M1', markersize=8)

    acc_adv_m2 = [s['diff_m3_m2'] for s in acc_summary]
    ax.plot(uncertainty, acc_adv_m2, marker='s', linewidth=2, color='blue', label='M3-M2', markersize=8)

    ax.axhline(0, color='black', linestyle='--', alpha=0.5)
    ax.set_xlabel('Environmental Uncertainty (1 - p_reward)')
    ax.set_ylabel('Accuracy Advantage')
    ax.set_title('M3 Advantage vs Noise (Accuracy)')
    ax.legend()
    ax.grid(True, alpha=0.3)

    # Plot 4: M3 Advantage vs Noise (LL)
    ax = axes[1, 0]
    ll_adv = [s['diff_m3_m1'] for s in ll_summary]
    ax.plot(uncertainty, ll_adv, marker='o', linewidth=2, color='green', label='M3-M1', markersize=8)

    ll_adv_m2 = [s['diff_m3_m2'] for s in ll_summary]
    ax.plot(uncertainty, ll_adv_m2, marker='s', linewidth=2, color='blue', label='M3-M2', markersize=8)

    ax.axhline(0, color='black', linestyle='--', alpha=0.5)
    ax.set_xlabel('Environmental Uncertainty (1 - p_reward)')
    ax.set_ylabel('Log-Likelihood Advantage')
    ax.set_title('M3 Advantage vs Noise (LL)')
    ax.legend()
    ax.grid(True, alpha=0.3)

    # Plot 5: Mean Gamma across noise levels
    ax = axes[1, 1]
    for model in ['M1', 'M2', 'M3']:
        gammas = [np.mean(results[model][nl['name']]['gamma']) for nl in noise_levels]
        ax.plot(uncertainty, gammas, marker='o', linewidth=2, label=model, markersize=8)

    ax.set_xlabel('Environmental Uncertainty (1 - p_reward)')
    ax.set_ylabel('Mean Policy Precision (γ)')
    ax.set_title('Precision Adaptation vs Noise')
    ax.legend()
    ax.grid(True, alpha=0.3)

    # Plot 6: Distribution at extreme noise levels
    ax = axes[1, 2]
    low_noise = noise_levels[0]['name']
    high_noise = noise_levels[-1]['name']

    positions = [1, 2, 3, 5, 6, 7]
    labels = ['M1\n(Low)', 'M2\n(Low)', 'M3\n(Low)', 'M1\n(High)', 'M2\n(High)', 'M3\n(High)']

    data = [
        results['M1'][low_noise]['accuracy'],
        results['M2'][low_noise]['accuracy'],
        results['M3'][low_noise]['accuracy'],
        results['M1'][high_noise]['accuracy'],
        results['M2'][high_noise]['accuracy'],
        results['M3'][high_noise]['accuracy'],
    ]

    bp = ax.boxplot(data, positions=positions, tick_labels=labels, patch_artist=True)

    colors = ['lightblue', 'lightgreen', 'lightcoral'] * 2
    for patch, color in zip(bp['boxes'], colors):
        patch.set_facecolor(color)

    ax.set_ylabel('Accuracy')
    ax.set_title('Performance: Low vs High Noise')
    ax.grid(True, alpha=0.3, axis='y')

    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"\nFigure saved to: {save_path}")

    plt.show()


# ============================================================================
# MAIN EXECUTION
# ============================================================================

print("="*70)
print("TESTING ROBUSTNESS HYPOTHESIS")
print("="*70)
print("""
Hypothesis: M3's advantage should GROW as environmental uncertainty increases.

Why? Because M3 can adaptively adjust precision based on belief uncertainty,
while M1 has fixed precision and M2 only has global adjustment.

If true, this confirms that state-dependent adaptive precision control is
the key mechanism behind M3's superior performance.
""")

# Run the test
results_noise = run_noise_robustness_test(
    n_seeds=15,
    trials=200,
    reversal_schedule=[30, 60, 90, 120, 150, 180],
    verbose=True
)

# Analyze results
acc_summary, ll_summary = analyze_noise_robustness(results_noise)

# Plot results
plot_noise_robustness(results_noise, acc_summary, ll_summary)

print("\n" + "="*70)
print("INTERPRETATION")
print("="*70)
print("""
If M3's advantage GROWS with noise:
  ✓ Confirms adaptive precision control is the key mechanism
  ✓ Shows M3 is more robust to environmental uncertainty
  ✓ Validates the theoretical motivation for state-dependent profiles

If M3's advantage is CONSTANT across noise:
  ~ M3 is generally better but not specifically because of adaptation

If M3's advantage SHRINKS with noise:
  ✗ Would suggest a different mechanism is responsible
  ✗ May indicate overfitting or brittleness
""")

"""### Mechanistic Validation Plots"""

# @title
"""
Policy Precision Dynamics (γ Over Time) Analysis

Shows how each model regulates policy precision across the task,
especially around reversals, revealing mechanistic differences.
"""

# ============================================================================
# POLICY PRECISION DYNAMICS ANALYSIS
# ============================================================================

def run_precision_dynamics_comparison(seed=42, trials=200, reversal_schedule=None, verbose=False):
    """
    Run all three models on the same environment to compare γ dynamics.

    Returns:
    --------
    trajectories : dict
        Contains trial-by-trial data for each model
    """

    if reversal_schedule is None:
        reversal_schedule = [50, 100, 150]

    print("="*70)
    print("POLICY PRECISION DYNAMICS ANALYSIS")
    print("="*70)
    print(f"\nRunning all three models with seed={seed}")
    print(f"Reversals at trials: {reversal_schedule}\n")

    trajectories = {
        'M1': None,
        'M2': None,
        'M3': None,
        'reversal_schedule': reversal_schedule,
        'seed': seed
    }

    for model_name in ['M1', 'M2', 'M3']:
        if verbose:
            print(f"Running {model_name}...")

        np.random.seed(seed)

        # Build environment
        env = TwoArmedBandit(
            probability_hint=0.7,
            probability_reward=0.8,
            reversal_schedule=reversal_schedule.copy()
        )

        # Build generative model
        A = build_A(num_modalities, state_contexts, state_choices, observation_hints, observation_rewards, observation_choices, 0.7, 0.8)
        B = build_B(state_contexts, state_choices, action_contexts, action_choices, context_volatility=0.05)
        D = build_D(state_contexts, state_choices)

        # Create model-specific value function
        if model_name == 'M1':
            value_fn = make_value_fn('M1', C_reward_logits=[0.0, -4.0, 2.0], gamma=1.2)

        elif model_name == 'M2':
            value_fn = make_value_fn('M2', C_reward_logits=[0.0, -4.0, 2.0],
                                    gamma_schedule=lambda q,t: gamma_entropy_coupled(q, t, g_base=1.6, k=1.0))

        elif model_name == 'M3':
            # Get policies
            A_temp = build_A(num_modalities, state_contexts, state_choices, observation_hints, observation_rewards, observation_choices, 0.7, 0.8)
            B_temp = build_B(state_contexts, state_choices, action_contexts, action_choices)
            D_temp = build_D(state_contexts, state_choices)
            C_temp = utils.obj_array_zeros([(A_temp[m].shape[0],) for m in range(len(A_temp))])

            temp_agent = Agent(A=A_temp, B=B_temp, C=C_temp, D=D_temp,
                             policy_len=2, inference_horizon=1,
                             control_fac_idx=[1], use_utility=True,
                             use_states_info_gain=True,
                             action_selection="stochastic", gamma=16)

            policies = temp_agent.policies
            num_actions_per_factor = [len(action_contexts), len(action_choices)]

            profiles = [
                {'phi_logits': [0.0, -2.0, 1.5], 'xi_logits': [0.0, 0.5, 0.0, 0.0], 'gamma': 0.6},
                {'phi_logits': [0.0, -8.0, 4.0], 'xi_logits': [0.0, -1.0, 0.0, 0.0], 'gamma': 3.0},
            ]

            Z = np.array([[0.8, 0.2], [0.2, 0.8]])

            value_fn = make_value_fn('M3', profiles=profiles, Z=Z,
                                    policies=policies,
                                    num_actions_per_factor=num_actions_per_factor)

        # Create agent runner
        runner = AgentRunner(A, B, D, value_fn,
                           observation_hints, observation_rewards,
                           observation_choices, action_choices,
                           reward_mod_idx=1)

        # Run episode
        logs = run_episode(runner, env, T=trials, verbose=False)

        # Store trajectory
        trajectories[model_name] = logs

    return trajectories


def analyze_precision_dynamics(trajectories, window_size=5):
    """
    Analyze γ dynamics around reversals.
    """

    reversals = trajectories['reversal_schedule']

    print("="*70)
    print("PRECISION DYNAMICS AROUND REVERSALS")
    print("="*70)

    for model_name in ['M1', 'M2', 'M3']:
        logs = trajectories[model_name]
        gamma_series = logs['gamma']

        print(f"\n{model_name}:")
        print(f"  Mean γ: {np.mean(gamma_series):.3f} (±{np.std(gamma_series):.3f})")
        print(f"  Range: [{np.min(gamma_series):.3f}, {np.max(gamma_series):.3f}]")

        # Analyze behavior around each reversal
        for rev_idx, rev_trial in enumerate(reversals):
            # Extract window around reversal
            start = max(0, rev_trial - window_size)
            end = min(len(gamma_series), rev_trial + window_size + 1)

            gamma_window = gamma_series[start:end]

            # Compute pre/post reversal means
            pre_rev = gamma_series[max(0, rev_trial-window_size):rev_trial]
            post_rev = gamma_series[rev_trial:min(len(gamma_series), rev_trial+window_size)]

            if len(pre_rev) > 0 and len(post_rev) > 0:
                mean_pre = np.mean(pre_rev)
                mean_post = np.mean(post_rev)
                delta = mean_post - mean_pre

                # Find minimum γ in post-reversal window (exploration dip)
                min_post = np.min(post_rev)
                min_idx = np.argmin(post_rev)

                print(f"  Reversal {rev_idx+1} (t={rev_trial}):")
                print(f"    Pre-reversal γ: {mean_pre:.3f}")
                print(f"    Post-reversal γ: {mean_post:.3f}")
                print(f"    Change: {delta:+.3f}")
                print(f"    Min post-rev γ: {min_post:.3f} (at t+{min_idx})")


def plot_precision_dynamics(trajectories, roll_k=7, save_path=None, figsize=(12, 5) ):
    """
    Create policy precision dynamics comparison
    """

    reversals = trajectories['reversal_schedule']

    fig, ax = plt.subplots(1, 1, figsize=figsize)

    colors = {
        'M1': '#3498db',  # Blue
        'M2': '#2ecc71',  # Green
        'M3': '#e74c3c'   # Red
    }

    # Plot all three models on same axis
    for model_name in ['M1', 'M2', 'M3']:
        logs = trajectories[model_name]
        gamma_series = logs['gamma']
        T = len(gamma_series)

        # Smoothed γ (prominent)
        gamma_smooth = rolling_mean(gamma_series, roll_k)
        ax.plot(gamma_smooth, color=colors[model_name],
                linewidth=3, label=model_name, alpha=0.85)

    # Mark reversals
    for rev in reversals:
        ax.axvline(rev, color='black', linestyle='--', linewidth=1.5, alpha=0.6)

    # Add reversal label
    ax.plot([], [], color='black', linestyle='--', linewidth=1.5, alpha=0.6, label='Reversal')

    ax.set_xlabel('Trial', fontsize=13, fontweight='bold')
    ax.set_ylabel('Policy Precision (γ)', fontsize=13, fontweight='bold')
    ax.set_title('Policy Precision Dynamics Across Models', fontsize=14, fontweight='bold', pad=15)
    ax.grid(True, alpha=0.3)
    ax.legend(loc='upper right', fontsize=11, framealpha=0.95)
    ax.set_xlim([0, T])

    # Add subtle background shading for reversal periods
    for rev in reversals:
        ax.axvspan(rev, rev+10, alpha=0.05, color='gray')

    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"\n Figure 3A saved to: {save_path}")

    plt.show()

    return fig, ax


def plot_reversal_analysis(trajectories, window=10, save_path=None):
    """
    Create detailed analysis of γ behavior around reversals.

    This provides supporting analysis for mechanistic interpretation.
    """

    reversals = trajectories['reversal_schedule']

    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    fig.suptitle('Precision Dynamics Around Reversals (Mechanistic Analysis)', fontsize=16, fontweight='bold')

    colors = {'M1': '#3498db', 'M2': '#2ecc71', 'M3': '#e74c3c'}

    # Plot 1: Average γ trajectory around reversals (NORMALIZED)
    ax = axes[0, 0]

    for model_name in ['M1', 'M2', 'M3']:
        logs = trajectories[model_name]
        gamma_series = logs['gamma']

        # Extract windows around each reversal and NORMALIZE by pre-reversal baseline
        rel_gammas = []
        for rev in reversals:
            start = max(0, rev - window)
            end = min(len(gamma_series), rev + window)
            seg = gamma_series[start:end]

            # Normalize by pre-reversal mean
            pre_mean = np.mean(gamma_series[max(0, rev-5):rev])
            if pre_mean > 0:
                seg_norm = seg / pre_mean  # Convert to proportion of baseline
            else:
                seg_norm = seg

            # Pad if needed
            pad_left = window - (rev - start)
            pad_right = window - (end - rev)
            seg_padded = np.pad(seg_norm, (pad_left, pad_right), constant_values=np.nan)

            rel_gammas.append(seg_padded)

        # Average across reversals
        rel_gammas = np.vstack(rel_gammas)
        mean_gamma = np.nanmean(rel_gammas, axis=0)
        stderr_gamma = np.nanstd(rel_gammas, axis=0) / np.sqrt(len(reversals))

        x = np.arange(-window, window)

        ax.plot(x, mean_gamma, color=colors[model_name],
                linewidth=2.5, label=model_name, marker='o', markersize=4, markevery=2)
        ax.fill_between(x, mean_gamma - stderr_gamma, mean_gamma + stderr_gamma,
                        color=colors[model_name], alpha=0.2)

    ax.axvline(0, color='black', linestyle='--', linewidth=2,
               alpha=0.7, label='Reversal')
    ax.axhline(1.0, color='gray', linestyle=':', linewidth=1, alpha=0.5)
    ax.set_xlabel('Trials Relative to Reversal', fontsize=11)
    ax.set_ylabel('γ (normalized to pre-reversal)', fontsize=11)
    ax.set_title('Normalized Precision Around Reversals', fontsize=12, fontweight='bold')
    ax.legend(loc='best')
    ax.grid(True, alpha=0.3)
    ax.set_ylim([0.4, 1.1])

    # Plot 2: Absolute γ change magnitude at each reversal
    ax = axes[0, 1]

    change_data = {model: [] for model in ['M1', 'M2', 'M3']}

    for model_name in ['M1', 'M2', 'M3']:
        logs = trajectories[model_name]
        gamma_series = logs['gamma']

        for rev in reversals:
            pre = gamma_series[max(0, rev-5):rev]
            # Look at immediate drop (first 3 trials)
            post = gamma_series[rev:min(len(gamma_series), rev+3)]

            if len(pre) > 0 and len(post) > 0:
                # Use the MINIMUM in the post window (biggest drop)
                change = np.min(post) - np.mean(pre)
                change_data[model_name].append(change)

    positions = [1, 2, 3]
    data = [change_data['M1'], change_data['M2'], change_data['M3']]

    bp = ax.boxplot(data, positions=positions, tick_labels=['M1', 'M2', 'M3'],
                   patch_artist=True, widths=0.6, showmeans=True,
                   meanprops=dict(marker='D', markerfacecolor='red', markersize=8))

    for patch, model in zip(bp['boxes'], ['M1', 'M2', 'M3']):
        patch.set_facecolor(colors[model])
        patch.set_alpha(0.7)

    # Add individual points
    for i, (model, d) in enumerate(zip(['M1', 'M2', 'M3'], data)):
        x = np.random.normal(i+1, 0.04, size=len(d))
        ax.scatter(x, d, alpha=0.6, s=50, color='black', zorder=3)

    ax.axhline(0, color='black', linestyle='-', linewidth=1, alpha=0.5)
    ax.set_ylabel('Δγ (min post - mean pre)', fontsize=11)
    ax.set_title('Maximum Precision Drop at Reversals', fontsize=12, fontweight='bold')
    ax.grid(True, alpha=0.3, axis='y')

    # Add text annotations for means
    for i, d in enumerate(data):
        mean_val = np.mean(d)
        ax.text(i+1, ax.get_ylim()[0] + 0.1, f'μ={mean_val:.3f}',
               ha='center', fontsize=9, bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

    # Plot 3: Belief entropy vs γ with trend lines
    ax = axes[1, 0]

    for model_name in ['M1', 'M2', 'M3']:
        logs = trajectories[model_name]
        gamma_series = logs['gamma']

        entropies = np.array([compute_entropy(b) for b in logs['belief']])
        gammas = np.array(gamma_series)

        # Subsample for clarity
        subsample_idx = np.arange(0, len(entropies), 2)

        ax.scatter(entropies[subsample_idx], gammas[subsample_idx],
                  color=colors[model_name], alpha=0.3, s=30, label=model_name)

        # Add trend line if there's variation
        if model_name in ['M2', 'M3']:
            # Bin entropies and compute mean gamma per bin
            bins = np.linspace(entropies.min(), entropies.max(), 10)
            bin_means_H = []
            bin_means_gamma = []

            for i in range(len(bins)-1):
                mask = (entropies >= bins[i]) & (entropies < bins[i+1])
                if mask.sum() > 5:
                    bin_means_H.append((bins[i] + bins[i+1]) / 2)
                    bin_means_gamma.append(gammas[mask].mean())

            if len(bin_means_H) > 0:
                ax.plot(bin_means_H, bin_means_gamma, color=colors[model_name],
                       linewidth=3, alpha=0.8, linestyle='-', marker='o', markersize=6)

    ax.set_xlabel('Belief Entropy H(q(context))', fontsize=11)
    ax.set_ylabel('γ (policy precision)', fontsize=11)
    ax.set_title('Precision vs Uncertainty (with trends)', fontsize=12, fontweight='bold')
    ax.legend()
    ax.grid(True, alpha=0.3)

    # Plot 4: Recovery time after reversals (FIXED)
    ax = axes[1, 1]

    recovery_data = {model: [] for model in ['M1', 'M2', 'M3']}

    for model_name in ['M1', 'M2', 'M3']:
        logs = trajectories[model_name]
        gamma_series = np.array(logs['gamma'])

        for rev in reversals:
            # Get pre-reversal baseline (mean of 5 trials before)
            pre_mean = np.mean(gamma_series[max(0, rev-5):rev])

            # Define recovery threshold: back to 90% of pre-reversal level
            threshold = pre_mean * 0.9

            # Look at post-reversal window
            post_window = gamma_series[rev:min(len(gamma_series), rev+25)]

            # Find first trial where γ exceeds threshold
            recovery_idx = np.where(post_window >= threshold)[0]

            if len(recovery_idx) > 0:
                recovery_time = recovery_idx[0]
                recovery_data[model_name].append(recovery_time)
            else:
                # If never recovers, mark as maximum window
                recovery_data[model_name].append(25)

    # Only plot if we have data
    valid_models = [m for m in ['M1', 'M2', 'M3'] if len(recovery_data[m]) > 0]

    if len(valid_models) > 0:
        positions = list(range(1, len(valid_models) + 1))
        data = [recovery_data[m] for m in valid_models]

        bp = ax.boxplot(data, positions=positions, tick_labels=valid_models,
                       patch_artist=True, widths=0.6, showmeans=True,
                       meanprops=dict(marker='D', markerfacecolor='red', markersize=8))

        for patch, model in zip(bp['boxes'], valid_models):
            patch.set_facecolor(colors[model])
            patch.set_alpha(0.7)

        # Add individual points
        for i, (model, d) in enumerate(zip(valid_models, data)):
            x = np.random.normal(i+1, 0.04, size=len(d))
            ax.scatter(x, d, alpha=0.6, s=50, color='black', zorder=3)

        ax.set_ylabel('Trials to Recovery (90% baseline)', fontsize=11)
        ax.set_title('Precision Recovery Speed', fontsize=12, fontweight='bold')
        ax.grid(True, alpha=0.3, axis='y')
        ax.set_ylim([-1, 26])

        # Add text annotations for means
        for i, d in enumerate(data):
            mean_val = np.mean(d)
            ax.text(i+1, -0.5, f'μ={mean_val:.1f}',
                   ha='center', fontsize=9, bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
    else:
        ax.text(0.5, 0.5, 'No recovery data available\n(models may not drop below threshold)',
               ha='center', va='center', transform=ax.transAxes, fontsize=11)
        ax.set_title('Precision Recovery Speed', fontsize=12, fontweight='bold')

    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"\n✓ Figure saved to: {save_path}")

    plt.show()


# ============================================================================
# MAIN EXECUTION
# ============================================================================

print("="*70)
print("MECHANISTIC VALIDATION: POLICY PRECISION DYNAMICS")
print("="*70)
print("""
This analysis reveals how each model regulates policy precision (γ)
over time, particularly around context reversals.

Expected patterns:
  • M1: Flat line (fixed γ)
  • M2: Gradual entropy-based fluctuations
  • M3: Abrupt dips and recoveries at reversals (profile switching)
""")

# Run comparison on same environment
trajectories = run_precision_dynamics_comparison(
    seed=42,
    trials=200,
    reversal_schedule=[50, 100, 150],
    verbose=True
)

# Analyze dynamics around reversals
analyze_precision_dynamics(trajectories, window_size=5)

# Generate main figure (Figure 3 for paper)
print("\n" + "="*70)
print("GENERATING FIGURE 3: POLICY PRECISION DYNAMICS")
print("="*70)
plot_precision_dynamics(trajectories, roll_k=7, save_path='figure3_precision_dynamics.png')

# Generate supplementary analysis
print("\n" + "="*70)
print("GENERATING SUPPLEMENTARY ANALYSIS")
print("="*70)
plot_reversal_analysis(trajectories, window=10, save_path='figure3_supp_reversal_analysis.png')

"""### Subject Parameter Fitting"""

"""
SUBJECT-LEVEL PARAMETER FITTING & MODEL COMPARISON
Implementation of proper hierarchical model fitting with cross-validation
"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.optimize import minimize
from scipy.special import softplus, expit
from scipy import stats
from sklearn.model_selection import KFold
import pandas as pd
from tqdm.notebook import tqdm
import warnings
import time
warnings.filterwarnings('ignore')

# ============================================================================
# PARAMETER TRANSFORMATIONS & PRIORS
# ============================================================================

def bootstrap_ci(data, n_bootstrap=1000, alpha=0.05):
    """
    Compute bootstrap confidence intervals.

    Parameters:
    -----------
    data : array-like
        Data to bootstrap
    n_bootstrap : int
        Number of bootstrap samples
    alpha : float
        Significance level (0.05 for 95% CI)

    Returns:
    --------
    mean, lower_ci, upper_ci : float
        Mean and confidence interval bounds
    """
    data = np.asarray(data)
    n = len(data)

    # Bootstrap samples
    bootstrap_means = []
    for _ in range(n_bootstrap):
        resample = np.random.choice(data, size=n, replace=True)
        bootstrap_means.append(np.mean(resample))

    bootstrap_means = np.array(bootstrap_means)

    # Compute percentiles
    lower_p = 100 * alpha / 2
    upper_p = 100 * (1 - alpha / 2)

    mean_est = np.mean(data)
    ci_lower = np.percentile(bootstrap_means, lower_p)
    ci_upper = np.percentile(bootstrap_means, upper_p)

    return mean_est, ci_lower, ci_upper

def softplus_inv(x):
    """Inverse softplus for initialization with numerical stability."""
    x = np.clip(x, 1e-8, 1e6)  # Prevent extreme values
    return np.log(np.exp(x) - 1.0 + 1e-8)

def transform_params(raw_params, model_name, num_profiles=2):
    """
    Transform unconstrained parameters to model space.

    Returns:
    --------
    params : dict
        Transformed parameters ready for model
    """
    params = {}
    idx = 0

    if model_name == 'M1':
        # M1: gamma (1), C_logits (2)
        params['gamma'] = softplus(raw_params[0]) + 0.1
        params['C_logits'] = raw_params[1:4]  # 3 outcomes

    elif model_name == 'M2':
        # M2: gamma_base (1), entropy_coupling (2), C_logits (2)
        params['gamma_base'] = softplus(raw_params[0]) + 0.1
        params['entropy_k'] = softplus(raw_params[1])
        params['C_logits'] = raw_params[2:5]

    elif model_name == 'M3':
        # M3: per-profile parameters + optional Z matrix
        # For 2 profiles: 2*(3 C + 4 xi + 1 gamma) = 16 params
        # Plus Z matrix: 2 free params (with sum-to-1 constraint)

        profiles = []
        for k in range(num_profiles):
            profile = {}
            profile['phi_logits'] = raw_params[idx:idx+3]  # C logits
            idx += 3
            profile['xi_logits'] = raw_params[idx:idx+4]   # E logits
            idx += 4
            # Enforce ordering: gamma_0 < gamma_1 < ...
            if k == 0:
                profile['gamma'] = softplus(raw_params[idx]) + 0.1
            else:
                # Each subsequent gamma must be larger
                profile['gamma'] = profiles[k-1]['gamma'] + softplus(raw_params[idx]) + 0.1
            idx += 1
            profiles.append(profile)

        params['profiles'] = profiles

        # Z matrix (2x2, row-constrained to sum to 1)
        # Parameterize as logits then softmax per row
        z_logits = raw_params[idx:idx+2]  # 2 free params for 2 rows
        Z = np.zeros((2, num_profiles))
        Z[0, :] = softmax(np.array([z_logits[0], 0.0]))  # first row
        Z[1, :] = softmax(np.array([0.0, z_logits[1]]))  # second row (flipped)
        params['Z'] = Z

    return params

def initialize_params(model_name, num_profiles=2):
    """Initialize parameters in unconstrained space."""

    if model_name == 'M1':
        # gamma, C_logits
        raw = np.concatenate([
            [softplus_inv(1.0)],  # gamma ~ 1.0
            [0.0, -3.0, 1.5]      # C logits (null, loss, reward)
        ])

    elif model_name == 'M2':
        # gamma_base, entropy_k, C_logits
        raw = np.concatenate([
            [softplus_inv(1.5)],   # gamma_base ~ 1.5
            [softplus_inv(1.0)],   # entropy_k ~ 1.0
            [0.0, -3.0, 1.5]       # C logits
        ])

    elif model_name == 'M3':
        # Profile 0: explore-ish
        raw_list = [
            [0.0, -2.0, 1.2],      # phi_logits
            [0.0, 0.3, 0.0, 0.0],  # xi_logits
            [softplus_inv(0.5)]    # gamma ~ 0.5
        ]
        # Profile 1: exploit-ish
        raw_list.append([0.0, -6.0, 3.0])      # phi_logits
        raw_list.append([0.0, -0.5, 0.0, 0.0]) # xi_logits
        raw_list.append([softplus_inv(1.5)])   # delta_gamma ~ 1.5 (total ~2.0)

        # Z matrix initialization (favoring diagonal)
        raw_list.append([1.5])  # Z[0,0] bias (left_better -> profile 0)
        raw_list.append([1.5])  # Z[1,1] bias (right_better -> profile 1)

        raw = np.concatenate([np.array(x).flatten() for x in raw_list])

    return raw

def log_prior(raw_params, model_name, num_profiles=2):
    """
    Compute log prior for regularization.

    Uses Gaussian priors centered at sensible defaults with moderate variance.
    """
    log_p = 0.0

    # Prior on gamma: prefer moderate values (not too low, not too high)
    # N(log(1.5), 1.0^2) in unconstrained space
    gamma_prior_mean = softplus_inv(1.5)
    gamma_prior_std = 1.0

    if model_name == 'M1':
        log_p += -0.5 * ((raw_params[0] - gamma_prior_mean) / gamma_prior_std) ** 2
        # Weak prior on C logits
        log_p += -0.5 * np.sum(raw_params[1:4] ** 2) / (3.0 ** 2)

    elif model_name == 'M2':
        log_p += -0.5 * ((raw_params[0] - gamma_prior_mean) / gamma_prior_std) ** 2
        log_p += -0.5 * ((raw_params[1]) / 1.0) ** 2  # entropy coupling
        log_p += -0.5 * np.sum(raw_params[2:5] ** 2) / (3.0 ** 2)

    elif model_name == 'M3':
        # Profile separation prior: encourage distinct gammas
        idx = 0
        gammas_raw = []
        for k in range(num_profiles):
            idx += 3 + 4  # skip phi and xi
            gammas_raw.append(raw_params[idx])
            idx += 1

        # Encourage positive spacing between profiles
        for i in range(len(gammas_raw)):
            if i == 0:
                log_p += -0.5 * ((gammas_raw[i] - softplus_inv(0.6)) / 1.0) ** 2
            else:
                # Encourage positive delta
                log_p += -0.5 * ((gammas_raw[i] - softplus_inv(1.0)) / 1.5) ** 2

        # Weak priors on phi, xi
        phi_xi_params = raw_params[:idx]
        log_p += -0.5 * np.sum(phi_xi_params ** 2) / (3.0 ** 2)

        # Prior on Z matrix (encourage diagonal-ish structure)
        z_logits = raw_params[idx:]
        log_p += -0.5 * np.sum((z_logits - 1.5) ** 2) / (2.0 ** 2)

    return log_p

def count_free_params(model_name, num_profiles=2):
    """Count number of free parameters for AIC/BIC."""
    if model_name == 'M1':
        return 1 + 3  # gamma + C_logits
    elif model_name == 'M2':
        return 2 + 3  # gamma_base, entropy_k + C_logits
    elif model_name == 'M3':
        # Per profile: 3 (phi) + 4 (xi) + 1 (gamma) = 8
        # Plus Z matrix: 2 free params
        return num_profiles * 8 + 2

# ============================================================================
# VALUE FUNCTION BUILDERS FROM PARAMETERS
# ============================================================================

def build_value_fn_from_params(params, model_name, policies=None, num_actions_per_factor=None):
    """Build value function from fitted parameters."""

    if model_name == 'M1':
        return make_value_fn('M1',
                            C_reward_logits=params['C_logits'],
                            gamma=params['gamma'])

    elif model_name == 'M2':
        def gamma_schedule(q, t):
            p = np.clip(np.asarray(q, float), 1e-12, 1.0)
            H = -(p * np.log(p)).sum()
            return params['gamma_base'] / (1.0 + params['entropy_k'] * H)

        return make_value_fn('M2',
                            C_reward_logits=params['C_logits'],
                            gamma_schedule=gamma_schedule)

    elif model_name == 'M3':
        return make_value_fn('M3',
                            profiles=params['profiles'],
                            Z=params['Z'],
                            policies=policies,
                            num_actions_per_factor=num_actions_per_factor)

# ============================================================================
# LIKELIHOOD COMPUTATION
# ============================================================================

def compute_trial_log_likelihood(runner, obs_ids, action_taken, t):
    """
    Compute log P(action_taken | observations, model).

    This is the core likelihood evaluation.
    """
    # Run inference
    qs = runner.agent.infer_states(obs_ids)
    q_context = qs[0]

    # Get value profile
    C_t, E_t, gamma_t = runner.value_fn(q_context, t)

    # Set agent parameters
    runner.agent.C[runner.reward_mod_idx] = C_t
    if E_t is not None and len(E_t) == len(runner.agent.policies):
        runner.agent.E = E_t
    runner.agent.gamma = float(gamma_t)

    # Infer policy posterior
    q_pi, _ = runner.agent.infer_policies()

    # Compute action probability by marginalizing over policies
    # Find which policies lead to this action at timestep 0
    action_prob = 0.0
    for pi_idx, policy in enumerate(runner.agent.policies):
        if int(policy[0, 1]) == action_taken:  # factor 1, timestep 0
            action_prob += q_pi[pi_idx]

    # Return log probability (with small epsilon for numerical stability)
    return np.log(action_prob + 1e-16)

# Global cache for expensive objects
_AGENT_CACHE = {}
_POLICIES_CACHE = None

def get_cached_policies():
    """Get cached policies to avoid recreating Agent objects."""
    global _POLICIES_CACHE
    if _POLICIES_CACHE is None:
        print("  Creating policy cache (one-time setup)...")
        A_temp = build_A(num_modalities, state_contexts, state_choices,
                       observation_hints, observation_rewards, observation_choices,
                       0.7, 0.8)
        B_temp = build_B(state_contexts, state_choices, action_contexts, action_choices)
        D_temp = build_D(state_contexts, state_choices)
        C_temp = utils.obj_array_zeros([(A_temp[m].shape[0],) for m in range(len(A_temp))])

        temp_agent = Agent(A=A_temp, B=B_temp, C=C_temp, D=D_temp,
                         policy_len=2, inference_horizon=1,
                         control_fac_idx=[1], use_utility=True,
                         use_states_info_gain=True,
                         action_selection="stochastic", gamma=16)

        _POLICIES_CACHE = {
            'policies': temp_agent.policies,
            'num_actions_per_factor': [len(action_contexts), len(action_choices)]
        }
        print(f"  ✓ Cached {len(temp_agent.policies)} policies")

    return _POLICIES_CACHE['policies'], _POLICIES_CACHE['num_actions_per_factor']

def compute_sequence_log_likelihood(raw_params, observations, actions, model_name,
                                   reversal_schedule, num_profiles=2, return_runner=False):
    """
    Compute log P(actions | observations, params) for entire sequence.

    This is the objective function we optimize.
    """
    try:
        # Transform parameters
        params = transform_params(raw_params, model_name, num_profiles)

        # Build environment (we don't actually use it, just for structure)
        env = TwoArmedBandit(
            probability_hint=0.7,
            probability_reward=0.8,
            reversal_schedule=reversal_schedule.copy()
        )

        # Build generative model
        A = build_A(num_modalities, state_contexts, state_choices,
                   observation_hints, observation_rewards, observation_choices,
                   0.7, 0.8)
        B = build_B(state_contexts, state_choices, action_contexts,
                   action_choices, context_volatility=0.05)
        D = build_D(state_contexts, state_choices)

        # Get policies if needed for M3
        if model_name == 'M3':
            policies, num_actions_per_factor = get_cached_policies()
        else:
            policies = None
            num_actions_per_factor = None

        # Build value function
        value_fn = build_value_fn_from_params(params, model_name, policies, num_actions_per_factor)

        # Create agent runner
        runner = AgentRunner(A, B, D, value_fn,
                           observation_hints, observation_rewards,
                           observation_choices, action_choices,
                           reward_mod_idx=1)

        # Compute log-likelihood trial by trial
        total_ll = 0.0

        # Initial observation
        obs_ids = runner.obs_labels_to_ids(observations[0])

        for t in range(len(actions)):
            # Get action index (factor 1)
            action_label = actions[t]
            action_idx = action_choices.index(action_label)

            # Compute log P(action_t | obs_1:t, params)
            ll_t = compute_trial_log_likelihood(runner, obs_ids, action_idx, t)
            total_ll += ll_t

            # Update for next trial (get next observation)
            if t + 1 < len(observations):
                obs_ids = runner.obs_labels_to_ids(observations[t + 1])

        # Add prior
        log_p = log_prior(raw_params, model_name, num_profiles)

        if return_runner:
            return total_ll + log_p, runner
        else:
            return total_ll + log_p

    except Exception as e:
        # Return very bad likelihood if computation fails
        if return_runner:
            return -1e10, None
        else:
            return -1e10

# ============================================================================
# FITTING FUNCTION
# ============================================================================

def fit_model(observations, actions, model_name, reversal_schedule,
              num_profiles=2, max_iter=500, verbose=False):
    """
    Fit model parameters to observed action sequence.

    Returns:
    --------
    result : dict
        Contains fitted parameters, log-likelihood, and convergence info
    """

    # Initialize parameters
    raw_init = initialize_params(model_name, num_profiles)

    # Set reasonable bounds to prevent numerical issues
    if model_name == 'M1':
        bounds = [(-5, 5)] * len(raw_init)  # Moderate bounds
    elif model_name == 'M2':
        bounds = [(-5, 5)] * len(raw_init)
    elif model_name == 'M3':
        bounds = [(-10, 10)] * len(raw_init)  # Slightly wider for complex model
    else:
        bounds = None

    # Objective function (negative log-likelihood for minimization)
    def objective(raw_params):
        ll = compute_sequence_log_likelihood(
            raw_params, observations, actions, model_name,
            reversal_schedule, num_profiles, return_runner=False
        )
        if not np.isfinite(ll):
            return 1e10  # Return large value for invalid likelihood
        return -ll  # Minimize negative LL

    # Optimize with multiple random starts for robustness
    best_result = None
    best_ll = -1e10

    # Reduced random starts for speed
    n_starts = 2 if model_name != 'M3' else 3  # Reduced from 3/5 to 2/3

    if verbose:
        print(f"  Starting {n_starts} optimization attempts...")

    for start in range(n_starts):
        # Add noise to initialization for multiple starts
        if start > 0:
            noise_scale = 0.5 if model_name != 'M3' else 0.3
            init_params = raw_init + np.random.normal(0, noise_scale, len(raw_init))
        else:
            init_params = raw_init.copy()

        if verbose:
            starting_ll = -objective(init_params)
            print(f"  Attempt {start+1}/{n_starts}: starting LL = {starting_ll:.2f}")

        try:
            start_time = time.time()
            result = minimize(
                objective,
                init_params,
                method='L-BFGS-B',
                bounds=bounds,
                options={'maxiter': 200, 'disp': False}  # Reduced from 500 to 200
            )
            opt_time = time.time() - start_time

            final_ll = -result.fun
            if verbose:
                print(f"  Attempt {start+1}: final LL = {final_ll:.2f}, "
                      f"success = {result.success}, time = {opt_time:.1f}s")

            if result.success and final_ll > best_ll:
                best_result = result
                best_ll = final_ll
                if verbose:
                    print(f"  ✓ New best result from attempt {start+1}")

        except Exception as e:
            if verbose:
                print(f"  ✗ Attempt {start+1} failed: {e}")
            continue

    if best_result is None:
        # Fallback if all starts failed
        if verbose:
            print(f"  All optimization starts failed for {model_name}")
        return {
            'model': model_name,
            'raw_params': raw_init,
            'params': transform_params(raw_init, model_name, num_profiles),
            'log_likelihood': -1e10,
            'aic': 1e10,
            'bic': 1e10,
            'n_params': count_free_params(model_name, num_profiles),
            'n_trials': len(actions),
            'success': False,
            'message': 'All optimization attempts failed'
        }

    # Transform to model space
    fitted_params = transform_params(best_result.x, model_name, num_profiles)

    # Compute final LL
    final_ll = -best_result.fun

    # Compute AIC/BIC
    n_trials = len(actions)
    k = count_free_params(model_name, num_profiles)
    aic = 2 * k - 2 * final_ll
    bic = k * np.log(n_trials) - 2 * final_ll

    return {
        'model': model_name,
        'raw_params': best_result.x,
        'params': fitted_params,
        'log_likelihood': final_ll,
        'aic': aic,
        'bic': bic,
        'n_params': k,
        'n_trials': n_trials,
        'success': best_result.success,
        'message': best_result.message
    }

# ============================================================================
# CROSS-VALIDATION
# ============================================================================

def blocked_cv(observations, actions, model_name, reversal_schedule,
               n_folds=3, num_profiles=2, verbose=False):  # Reduced folds from 5 to 3
    """
    Perform blocked K-fold cross-validation.

    Splits data into contiguous blocks to avoid leakage around reversals.
    """

    n_trials = len(actions)
    block_size = n_trials // n_folds

    cv_scores = []

    if verbose:
        print(f"  Running {n_folds}-fold cross-validation...")

    for fold in range(n_folds):
        if verbose:
            print(f"  Fold {fold+1}/{n_folds}...", end=" ")

        # Define train/test split
        test_start = fold * block_size
        test_end = (fold + 1) * block_size if fold < n_folds - 1 else n_trials

        # Create train/test masks
        train_mask = np.ones(n_trials, dtype=bool)
        train_mask[test_start:test_end] = False

        # Extract train data
        obs_train = [observations[i] for i in range(n_trials) if train_mask[i]]
        act_train = [actions[i] for i in range(n_trials) if train_mask[i]]

        # Adjust reversal schedule for train set - only include reversals in training data
        rev_train = []
        train_indices = np.where(train_mask)[0]
        for r in reversal_schedule:
            if r in train_indices:
                # Find position in training data
                rev_train_pos = np.where(train_indices == r)[0]
                if len(rev_train_pos) > 0:
                    rev_train.append(rev_train_pos[0])

        # Fit on train
        try:
            fit_result = fit_model(obs_train, act_train, model_name, rev_train,
                                  num_profiles, verbose=False)

            # Evaluate on test
            obs_test = [observations[i] for i in range(test_start, test_end)]
            act_test = [actions[i] for i in range(test_start, test_end)]
            rev_test = [r - test_start for r in reversal_schedule
                       if test_start <= r < test_end]

            test_ll = compute_sequence_log_likelihood(
                fit_result['raw_params'], obs_test, act_test, model_name,
                rev_test, num_profiles, return_runner=False
            )

            cv_scores.append(test_ll)

            if verbose:
                print(f"LL={test_ll:.2f}")

        except Exception as e:
            if verbose:
                print(f"FAILED ({e})")
            cv_scores.append(-1e10)

    return np.array(cv_scores)

# ============================================================================
# PARAMETER RECOVERY
# ============================================================================

def parameter_recovery(true_params, model_name, reversal_schedule,
                      n_sims=10, n_trials=200, num_profiles=2, verbose=False):
    """
    Test parameter recovery by simulating from fitted params and refitting.
    """

    recovered_params = []

    for sim in range(n_sims):
        if verbose and sim % 5 == 0:
            print(f"Recovery simulation {sim+1}/{n_sims}")

        # Simulate data from true parameters
        np.random.seed(sim)

        # Build environment
        env = TwoArmedBandit(
            probability_hint=0.7,
            probability_reward=0.8,
            reversal_schedule=reversal_schedule.copy()
        )

        # Build model with true parameters
        A = build_A(num_modalities, state_contexts, state_choices,
                   observation_hints, observation_rewards, observation_choices,
                   0.7, 0.8)
        B = build_B(state_contexts, state_choices, action_contexts,
                   action_choices, context_volatility=0.05)
        D = build_D(state_contexts, state_choices)

        # Get policies if needed
        if model_name == 'M3':
            A_temp = build_A(num_modalities, state_contexts, state_choices,
                           observation_hints, observation_rewards, observation_choices,
                           0.7, 0.8)
            B_temp = build_B(state_contexts, state_choices, action_contexts, action_choices)
            D_temp = build_D(state_contexts, state_choices)
            C_temp = utils.obj_array_zeros([(A_temp[m].shape[0],) for m in range(len(A_temp))])

            temp_agent = Agent(A=A_temp, B=B_temp, C=C_temp, D=D_temp,
                             policy_len=2, inference_horizon=1,
                             control_fac_idx=[1], use_utility=True,
                             use_states_info_gain=True,
                             action_selection="stochastic", gamma=16)

            policies = temp_agent.policies
            num_actions_per_factor = [len(action_contexts), len(action_choices)]
        else:
            policies = None
            num_actions_per_factor = None

        # Build value function from true parameters
        value_fn = build_value_fn_from_params(true_params, model_name,
                                              policies, num_actions_per_factor)

        # Create runner and simulate
        runner = AgentRunner(A, B, D, value_fn,
                           observation_hints, observation_rewards,
                           observation_choices, action_choices,
                           reward_mod_idx=1)

        # Run episode
        logs = run_episode(runner, env, T=n_trials, verbose=False)

        # Extract observations and actions
        observations = [['null', 'null', 'observe_start']]  # Initial
        for t in range(len(logs['action'])):
            observations.append([
                'null',  # hint not directly observed
                logs['reward_label'][t],
                logs['choice_label'][t]
            ])

        actions = logs['action']

        # Refit to simulated data
        try:
            fit_result = fit_model(observations, actions, model_name,
                                  reversal_schedule, num_profiles, verbose=False)
            recovered_params.append(fit_result['params'])
        except:
            recovered_params.append(None)

    return recovered_params

# ============================================================================
# MAIN EXPERIMENT RUNNER
# ============================================================================

def run_subject_level_experiment(n_subjects=10, n_trials=200,
                                 reversal_schedule=None, verbose=True,
                                 skip_cv=False):  # Add option to skip CV
    """
    Main experiment: generate synthetic subjects, fit models, compare.
    """

    if reversal_schedule is None:
        reversal_schedule = [50, 100, 150]

    print("="*70)
    print("SUBJECT-LEVEL PARAMETER FITTING EXPERIMENT")
    print("="*70)
    print(f"\nGenerating {n_subjects} synthetic subjects")
    print(f"Trials per subject: {n_trials}")
    print(f"Reversal schedule: {reversal_schedule}")
    print(f"Total model fits planned: {n_subjects * 3}")
    if skip_cv:
        print("⚡ FAST MODE: Cross-validation disabled")
        print(f"Estimated runtime: {n_subjects * 1:.1f}-{n_subjects * 2:.1f} minutes")
    else:
        print(f"Estimated runtime: {n_subjects * 2:.1f}-{n_subjects * 5:.1f} minutes")
    print()

    experiment_start_time = time.time()

    # Storage for results
    all_results = {
        'subject_id': [],
        'model': [],
        'log_likelihood': [],
        'cv_log_likelihood': [],
        'aic': [],
        'bic': [],
        'n_params': [],
        'fitted_params': [],
        'raw_params': []
    }

    # Generate subjects and fit models
    for subj in tqdm(range(n_subjects), desc="Subjects"):
        subject_start_time = time.time()
        print(f"\n{'='*50}")
        print(f"PROCESSING SUBJECT {subj+1}/{n_subjects}")

        # Estimate time remaining
        if subj > 0:
            elapsed = time.time() - experiment_start_time
            avg_time_per_subject = elapsed / subj
            estimated_remaining = avg_time_per_subject * (n_subjects - subj)
            print(f"Elapsed: {elapsed/60:.1f} min | Est. remaining: {estimated_remaining/60:.1f} min")

        print(f"{'='*50}")

        # Generate "ground truth" data from M3 (most complex model)
        print(f"[Subject {subj+1}] Generating synthetic data from M3...")
        np.random.seed(subj * 100)

        # Build environment
        env = TwoArmedBandit(
            probability_hint=0.7,
            probability_reward=0.8,
            reversal_schedule=reversal_schedule.copy()
        )

        # Use M3 to generate data (ground truth has profiles)
        A = build_A(num_modalities, state_contexts, state_choices,
                   observation_hints, observation_rewards, observation_choices,
                   0.7, 0.8)
        B = build_B(state_contexts, state_choices, action_contexts,
                   action_choices, context_volatility=0.05)
        D = build_D(state_contexts, state_choices)

        # Get policies
        A_temp = build_A(num_modalities, state_contexts, state_choices,
                       observation_hints, observation_rewards, observation_choices,
                       0.7, 0.8)
        B_temp = build_B(state_contexts, state_choices, action_contexts, action_choices)
        D_temp = build_D(state_contexts, state_choices)
        C_temp = utils.obj_array_zeros([(A_temp[m].shape[0],) for m in range(len(A_temp))])

        temp_agent = Agent(A=A_temp, B=B_temp, C=C_temp, D=D_temp,
                         policy_len=2, inference_horizon=1,
                         control_fac_idx=[1], use_utility=True,
                         use_states_info_gain=True,
                         action_selection="stochastic", gamma=16)

        policies = temp_agent.policies
        num_actions_per_factor = [len(action_contexts), len(action_choices)]

        # Generate with some variability across subjects
        profiles = [
            {'phi_logits': [0.0, -2.0 + np.random.randn()*0.5, 1.5 + np.random.randn()*0.3],
             'xi_logits': [0.0, 0.5, 0.0, 0.0],
             'gamma': 0.6 + np.random.rand()*0.3},
            {'phi_logits': [0.0, -8.0 + np.random.randn()*1.0, 4.0 + np.random.randn()*0.5],
             'xi_logits': [0.0, -1.0, 0.0, 0.0],
             'gamma': 2.5 + np.random.rand()*1.0},
        ]

        Z = np.array([[0.8, 0.2], [0.2, 0.8]])

        value_fn = make_value_fn('M3', profiles=profiles, Z=Z,
                                policies=policies,
                                num_actions_per_factor=num_actions_per_factor)

        # Generate data
        print(f"[Subject {subj+1}] Running {n_trials} trial simulation...")
        runner = AgentRunner(A, B, D, value_fn,
                           observation_hints, observation_rewards,
                           observation_choices, action_choices,
                           reward_mod_idx=1)

        logs = run_episode(runner, env, T=n_trials, verbose=False)
        print(f"[Subject {subj+1}] ✓ Simulation complete")

        # Extract observations and actions
        print(f"[Subject {subj+1}] Extracting behavioral data...")
        observations = [['null', 'null', 'observe_start']]
        for t in range(len(logs['action'])):
            observations.append([
                'null',
                logs['reward_label'][t],
                logs['choice_label'][t]
            ])

        actions = logs['action']
        print(f"[Subject {subj+1}] ✓ Extracted {len(actions)} actions")

        # Fit each model to this subject's data
        print(f"[Subject {subj+1}] Starting model fitting...")
        for model_idx, model_name in enumerate(['M1', 'M2', 'M3']):
            print(f"[Subject {subj+1}] Fitting {model_name} ({model_idx+1}/3)...")

            # Full fit
            fit_start_time = time.time()
            fit_result = fit_model(observations, actions, model_name,
                                  reversal_schedule, num_profiles=2,
                                  verbose=False)
            fit_time = time.time() - fit_start_time

            success_str = "✓" if fit_result['success'] else "✗"
            print(f"[Subject {subj+1}] {model_name} fit {success_str}: "
                  f"LL={fit_result['log_likelihood']:.2f}, "
                  f"time={fit_time:.1f}s")

            # Cross-validation (optional for speed)
            if not skip_cv:
                print(f"[Subject {subj+1}] Running {model_name} cross-validation...")
                cv_start_time = time.time()
                cv_scores = blocked_cv(observations, actions, model_name,
                                      reversal_schedule, n_folds=3,  # Reduced from 5 to 3
                                      num_profiles=2, verbose=False)
                cv_time = time.time() - cv_start_time
                cv_mean = np.mean(cv_scores)
                print(f"[Subject {subj+1}] {model_name} CV: "
                      f"mean_LL={cv_mean:.2f}, time={cv_time:.1f}s")
            else:
                # Use training LL as proxy for CV LL
                cv_mean = fit_result['log_likelihood']
                print(f"[Subject {subj+1}] {model_name} CV: SKIPPED (using train LL={cv_mean:.2f})")

            # Store results
            all_results['subject_id'].append(subj)
            all_results['model'].append(model_name)
            all_results['log_likelihood'].append(fit_result['log_likelihood'])
            all_results['cv_log_likelihood'].append(cv_mean)
            all_results['aic'].append(fit_result['aic'])
            all_results['bic'].append(fit_result['bic'])
            all_results['n_params'].append(fit_result['n_params'])
            all_results['fitted_params'].append(fit_result['params'])
            all_results['raw_params'].append(fit_result['raw_params'])

        print(f"[Subject {subj+1}] ✓ All models completed successfully!")

        # Show quick summary
        subj_results = [(model, all_results['cv_log_likelihood'][-3:][i])
                       for i, model in enumerate(['M1', 'M2', 'M3'])]
        print(f"[Subject {subj+1}] CV-LL summary: " +
              ", ".join([f"{m}={ll:.2f}" for m, ll in subj_results]))

        subject_time = time.time() - subject_start_time
        print(f"[Subject {subj+1}] Subject completed in {subject_time:.1f}s")

    total_time = time.time() - experiment_start_time
    print(f"\n{'='*70}")
    print("ALL SUBJECTS COMPLETED!")
    print(f"Total subjects: {n_subjects}")
    print(f"Total model fits: {n_subjects * 3}")
    print(f"Total runtime: {total_time/60:.1f} minutes")
    print(f"Average per subject: {total_time/n_subjects:.1f}s")
    print(f"{'='*70}")

    return pd.DataFrame(all_results)

# ============================================================================
# ANALYSIS & VISUALIZATION
# ============================================================================

def analyze_and_plot_results(df, save_prefix='subject_fit'):
    """
    Analyze fitted results and create publication-quality figures.
    """

    print("\n" + "="*70)
    print("ANALYSIS OF FITTED RESULTS")
    print("="*70)
    print("Starting statistical analysis and visualization...")

    # Table 1: Summary statistics
    print("\nStep 1/4: Computing summary statistics...")
    print("\nTable 1: Model Comparison Summary")
    print("-"*70)

    summary = df.groupby('model').agg({
        'cv_log_likelihood': ['mean', 'std'],
        'aic': ['mean', 'std'],
        'bic': ['mean', 'std']
    }).round(2)

    print(summary)

    # Compute deltas relative to M3
    print("\nStep 2/4: Computing statistical comparisons...")
    print("\n" + "-"*70)
    print("Differences from M3 (M3 - other)")
    print("-"*70)

    for metric in ['cv_log_likelihood', 'aic', 'bic']:
        print(f"\n{metric}:")

        # Get M3 values per subject
        m3_values = df[df['model'] == 'M3'].set_index('subject_id')[metric]

        for model in ['M1', 'M2']:
            model_values = df[df['model'] == model].set_index('subject_id')[metric]

            if metric == 'cv_log_likelihood':
                delta = m3_values - model_values  # M3 higher is better
            else:  # AIC/BIC
                delta = model_values - m3_values  # M3 lower is better, so flip

            mean_delta = delta.mean()
            ci_low, ci_high = bootstrap_ci(delta.values)[1:]

            # Paired t-test
            t_stat, p_val = stats.ttest_rel(m3_values, model_values)

            print(f"  M3 vs {model}: Δ = {mean_delta:.2f} "
                  f"(95% CI: [{ci_low:.2f}, {ci_high:.2f}]), p = {p_val:.4f}")

    # Figure 1: CV Log-Likelihood Raincloud Plot
    print("\nStep 3/4: Generating main comparison figures...")
    print("\n" + "="*70)
    print("GENERATING FIGURES")
    print("="*70)

    fig, axes = plt.subplots(1, 3, figsize=(15, 5))
    fig.suptitle('Subject-Level Model Comparison', fontsize=16, fontweight='bold')

    # Plot 1: CV-LL distributions
    ax = axes[0]

    # Reshape data for paired visualization
    pivot_cv = df.pivot(index='subject_id', columns='model', values='cv_log_likelihood')

    positions = [1, 2, 3]
    data = [pivot_cv['M1'].values, pivot_cv['M2'].values, pivot_cv['M3'].values]
    colors_box = ['#3498db', '#2ecc71', '#e74c3c']

    bp = ax.boxplot(data, positions=positions, widths=0.5, patch_artist=True,
                    showmeans=True, meanprops=dict(marker='D', markerfacecolor='black', markersize=8))

    for patch, color in zip(bp['boxes'], colors_box):
        patch.set_facecolor(color)
        patch.set_alpha(0.7)

    # Add individual subject lines
    for i in range(len(pivot_cv)):
        ax.plot([1, 2, 3], [pivot_cv.iloc[i]['M1'], pivot_cv.iloc[i]['M2'],
                            pivot_cv.iloc[i]['M3']],
                color='gray', alpha=0.3, linewidth=0.8, zorder=1)

    ax.set_xlim(0.5, 3.5)
    ax.set_xticks([1, 2, 3])
    ax.set_xticklabels(['M1', 'M2', 'M3'])
    ax.set_ylabel('CV Log-Likelihood')
    ax.set_title('Cross-Validated Performance')
    ax.grid(True, alpha=0.3)

    # Plot 2: AIC differences
    ax = axes[1]

    # Compute AIC differences relative to M3 (lower is better)
    pivot_aic = df.pivot(index='subject_id', columns='model', values='aic')
    aic_diff_m1 = pivot_aic['M1'] - pivot_aic['M3']  # Positive = M1 worse
    aic_diff_m2 = pivot_aic['M2'] - pivot_aic['M3']  # Positive = M2 worse

    ax.scatter(np.ones(len(aic_diff_m1)) + np.random.normal(0, 0.05, len(aic_diff_m1)),
               aic_diff_m1, alpha=0.6, color='#3498db', s=40, label='M1 - M3')
    ax.scatter(np.ones(len(aic_diff_m2)) * 2 + np.random.normal(0, 0.05, len(aic_diff_m2)),
               aic_diff_m2, alpha=0.6, color='#2ecc71', s=40, label='M2 - M3')

    # Add horizontal line at 0 (equal performance)
    ax.axhline(0, color='red', linestyle='--', alpha=0.7, label='Equal to M3')

    # Add means and error bars
    mean_m1 = np.mean(aic_diff_m1)
    mean_m2 = np.mean(aic_diff_m2)
    se_m1 = np.std(aic_diff_m1) / np.sqrt(len(aic_diff_m1))
    se_m2 = np.std(aic_diff_m2) / np.sqrt(len(aic_diff_m2))

    ax.errorbar([1], [mean_m1], yerr=[se_m1], color='#3498db',
                capsize=5, marker='D', markersize=8, linewidth=2)
    ax.errorbar([2], [mean_m2], yerr=[se_m2], color='#2ecc71',
                capsize=5, marker='D', markersize=8, linewidth=2)

    ax.set_xlim(0.5, 2.5)
    ax.set_xticks([1, 2])
    ax.set_xticklabels(['M1 - M3', 'M2 - M3'])
    ax.set_ylabel('ΔAIC (higher = worse)')
    ax.set_title('AIC Differences from M3')
    ax.grid(True, alpha=0.3)
    ax.legend()

    # Plot 3: Parameter distributions for M3
    ax = axes[2]

    # Extract M3 parameters
    m3_data = df[df['model'] == 'M3']
    gamma_0 = [p['profiles'][0]['gamma'] for p in m3_data['fitted_params']]
    gamma_1 = [p['profiles'][1]['gamma'] for p in m3_data['fitted_params']]

    ax.scatter(gamma_0, gamma_1, alpha=0.7, color='#e74c3c', s=60)
    ax.plot([0, 4], [0, 4], '--', color='gray', alpha=0.5, label='γ₀ = γ₁')

    ax.set_xlabel('γ₀ (Profile 0)')
    ax.set_ylabel('γ₁ (Profile 1)')
    ax.set_title('M3: Profile Precision Parameters')
    ax.grid(True, alpha=0.3)
    ax.legend()

    # Force γ₁ > γ₀ constraint visualization
    ax.fill_between([0, 4], [0, 4], [4, 4], alpha=0.1, color='red',
                    label='Expected region (γ₁ > γ₀)')

    plt.tight_layout()
    plt.savefig(f'{save_prefix}_model_comparison.png', dpi=300, bbox_inches='tight')
    print(f"✓ Saved: {save_prefix}_model_comparison.png")
    plt.show()

    print("\nStep 4/4: Generating parameter distribution plots...")
    return df

    # Add scatter
    for i, (pos, d) in enumerate(zip(positions, data)):
        x = np.random.normal(pos, 0.04, size=len(d))
        ax.scatter(x, d, alpha=0.6, s=40, color='black', zorder=3)

    ax.set_xticks(positions)
    ax.set_xticklabels(['M1', 'M2', 'M3'])
    ax.set_ylabel('CV Log-Likelihood', fontsize=12, fontweight='bold')
    ax.set_title('Cross-Validated Performance', fontsize=13, fontweight='bold')
    ax.grid(True, alpha=0.3, axis='y')

    # Plot 2: Delta LL (M3 - others)
    ax = axes[1]

    delta_m1 = pivot_cv['M3'] - pivot_cv['M1']
    delta_m2 = pivot_cv['M3'] - pivot_cv['M2']

    bp = ax.boxplot([delta_m1.values, delta_m2.values],
                    positions=[1, 2], widths=0.5, patch_artist=True,
                    showmeans=True, meanprops=dict(marker='D', markerfacecolor='red', markersize=8))

    for patch in bp['boxes']:
        patch.set_facecolor('lightcoral')
        patch.set_alpha(0.7)

    # Add individual points
    for i, (pos, d) in enumerate(zip([1, 2], [delta_m1.values, delta_m2.values])):
        x = np.random.normal(pos, 0.04, size=len(d))
        ax.scatter(x, d, alpha=0.6, s=40, color='black', zorder=3)

    ax.axhline(0, color='black', linestyle='--', linewidth=1.5, alpha=0.6)
    ax.set_xticks([1, 2])
    ax.set_xticklabels(['M3 - M1', 'M3 - M2'])
    ax.set_ylabel('ΔCV-LL', fontsize=12, fontweight='bold')
    ax.set_title('M3 Advantage', fontsize=13, fontweight='bold')
    ax.grid(True, alpha=0.3, axis='y')

    # Add significance markers
    for i, (name, delta) in enumerate([('M3-M1', delta_m1), ('M3-M2', delta_m2)]):
        from scipy.stats import ttest_1samp
        _, p = ttest_1samp(delta.values, 0)
        if p < 0.001:
            sig = '***'
        elif p < 0.01:
            sig = '**'
        elif p < 0.05:
            sig = '*'
        else:
            sig = 'ns'

        y_max = ax.get_ylim()[1]
        ax.text(i+1, y_max * 0.9, sig, ha='center', fontsize=14, fontweight='bold')

    # Plot 3: BIC comparison
    ax = axes[2]

    pivot_bic = df.pivot(index='subject_id', columns='model', values='bic')

    data_bic = [pivot_bic['M1'].values, pivot_bic['M2'].values, pivot_bic['M3'].values]

    bp = ax.boxplot(data_bic, positions=positions, widths=0.5, patch_artist=True,
                    showmeans=True, meanprops=dict(marker='D', markerfacecolor='black', markersize=8))

    for patch, color in zip(bp['boxes'], colors_box):
        patch.set_facecolor(color)
        patch.set_alpha(0.7)

    # Add individual subject lines
    for i in range(len(pivot_bic)):
        ax.plot([1, 2, 3], [pivot_bic.iloc[i]['M1'], pivot_bic.iloc[i]['M2'],
                            pivot_bic.iloc[i]['M3']],
                color='gray', alpha=0.3, linewidth=0.8, zorder=1)

    for i, (pos, d) in enumerate(zip(positions, data_bic)):
        x = np.random.normal(pos, 0.04, size=len(d))
        ax.scatter(x, d, alpha=0.6, s=40, color='black', zorder=3)

    ax.set_xticks(positions)
    ax.set_xticklabels(['M1', 'M2', 'M3'])
    ax.set_ylabel('BIC', fontsize=12, fontweight='bold')
    ax.set_title('Bayesian Information Criterion\n(lower is better)', fontsize=13, fontweight='bold')
    ax.grid(True, alpha=0.3, axis='y')

    plt.tight_layout()
    plt.savefig(f'{save_prefix}_comparison.png', dpi=300, bbox_inches='tight')
    print(f"✓ Saved: {save_prefix}_comparison.png")
    plt.show()

    # Figure 2: Parameter distributions for M3
    print("\nGenerating parameter distribution plots...")

    m3_data = df[df['model'] == 'M3']

    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    fig.suptitle('M3 Fitted Parameter Distributions', fontsize=16, fontweight='bold')

    # Extract gamma values for both profiles
    gamma_0 = [p['profiles'][0]['gamma'] for p in m3_data['fitted_params']]
    gamma_1 = [p['profiles'][1]['gamma'] for p in m3_data['fitted_params']]

    # Plot gamma distributions
    ax = axes[0, 0]
    ax.hist(gamma_0, bins=15, alpha=0.6, color='steelblue', label='Profile 0 (explore)', edgecolor='black')
    ax.hist(gamma_1, bins=15, alpha=0.6, color='coral', label='Profile 1 (exploit)', edgecolor='black')
    ax.axvline(np.mean(gamma_0), color='steelblue', linestyle='--', linewidth=2)
    ax.axvline(np.mean(gamma_1), color='coral', linestyle='--', linewidth=2)
    ax.set_xlabel('γ (policy precision)', fontsize=11)
    ax.set_ylabel('Count', fontsize=11)
    ax.set_title('Profile Precisions', fontsize=12, fontweight='bold')
    ax.legend()
    ax.grid(True, alpha=0.3)

    # Plot gamma scatter (ordering)
    ax = axes[0, 1]
    ax.scatter(gamma_0, gamma_1, s=60, alpha=0.6, color='purple', edgecolor='black')
    ax.plot([0, 4], [0, 4], 'k--', alpha=0.5, label='γ₀ = γ₁')
    ax.set_xlabel('γ₀ (Profile 0)', fontsize=11)
    ax.set_ylabel('γ₁ (Profile 1)', fontsize=11)
    ax.set_title('Profile Ordering', fontsize=12, fontweight='bold')
    ax.legend()
    ax.grid(True, alpha=0.3)

    # Extract phi_logits (reward preferences)
    phi_0_reward = [p['profiles'][0]['phi_logits'][2] for p in m3_data['fitted_params']]
    phi_1_reward = [p['profiles'][1]['phi_logits'][2] for p in m3_data['fitted_params']]

    ax = axes[0, 2]
    ax.hist(phi_0_reward, bins=15, alpha=0.6, color='steelblue', label='Profile 0', edgecolor='black')
    ax.hist(phi_1_reward, bins=15, alpha=0.6, color='coral', label='Profile 1', edgecolor='black')
    ax.axvline(np.mean(phi_0_reward), color='steelblue', linestyle='--', linewidth=2)
    ax.axvline(np.mean(phi_1_reward), color='coral', linestyle='--', linewidth=2)
    ax.set_xlabel('Reward logit', fontsize=11)
    ax.set_ylabel('Count', fontsize=11)
    ax.set_title('Reward Preferences', fontsize=12, fontweight='bold')
    ax.legend()
    ax.grid(True, alpha=0.3)

    # Extract Z matrix values
    z_00 = [p['Z'][0, 0] for p in m3_data['fitted_params']]  # left_better -> profile 0
    z_11 = [p['Z'][1, 1] for p in m3_data['fitted_params']]  # right_better -> profile 1

    ax = axes[1, 0]
    ax.hist(z_00, bins=15, alpha=0.6, color='steelblue', label='Z[left→P0]', edgecolor='black')
    ax.hist(z_11, bins=15, alpha=0.6, color='coral', label='Z[right→P1]', edgecolor='black')
    ax.axvline(np.mean(z_00), color='steelblue', linestyle='--', linewidth=2)
    ax.axvline(np.mean(z_11), color='coral', linestyle='--', linewidth=2)
    ax.set_xlabel('Assignment weight', fontsize=11)
    ax.set_ylabel('Count', fontsize=11)
    ax.set_title('Profile Assignments (Z)', fontsize=12, fontweight='bold')
    ax.legend()
    ax.grid(True, alpha=0.3)

    # Subject-level variability
    ax = axes[1, 1]
    subjects = m3_data['subject_id'].values
    ax.scatter(subjects, gamma_0, s=60, alpha=0.7, color='steelblue', label='γ₀', marker='o')
    ax.scatter(subjects, gamma_1, s=60, alpha=0.7, color='coral', label='γ₁', marker='s')
    ax.plot(subjects, [np.mean(gamma_0)] * len(subjects), 'steelblue', linestyle='--', linewidth=1.5)
    ax.plot(subjects, [np.mean(gamma_1)] * len(subjects), 'coral', linestyle='--', linewidth=1.5)
    ax.set_xlabel('Subject ID', fontsize=11)
    ax.set_ylabel('γ', fontsize=11)
    ax.set_title('Inter-Subject Variability', fontsize=12, fontweight='bold')
    ax.legend()
    ax.grid(True, alpha=0.3)

    # Correlation: gamma_1 - gamma_0 vs CV-LL
    ax = axes[1, 2]
    gamma_diff = np.array(gamma_1) - np.array(gamma_0)
    cv_ll = m3_data['cv_log_likelihood'].values

    ax.scatter(gamma_diff, cv_ll, s=60, alpha=0.6, color='purple', edgecolor='black')

    # Add trend line
    z = np.polyfit(gamma_diff, cv_ll, 1)
    p = np.poly1d(z)
    x_line = np.linspace(gamma_diff.min(), gamma_diff.max(), 100)
    ax.plot(x_line, p(x_line), 'r--', linewidth=2, alpha=0.7)

    from scipy.stats import pearsonr
    r, p_val = pearsonr(gamma_diff, cv_ll)
    ax.text(0.05, 0.95, f'r = {r:.3f}, p = {p_val:.3f}',
            transform=ax.transAxes, fontsize=10, verticalalignment='top',
            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

    ax.set_xlabel('γ₁ - γ₀ (separation)', fontsize=11)
    ax.set_ylabel('CV Log-Likelihood', fontsize=11)
    ax.set_title('Profile Separation vs Performance', fontsize=12, fontweight='bold')
    ax.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(f'{save_prefix}_parameters_m3.png', dpi=300, bbox_inches='tight')
    print(f"✓ Saved: {save_prefix}_parameters_m3.png")
    plt.show()

def trial_accuracy(actions, contexts):
    """
    Compute trial-by-trial accuracy given actions and true contexts.

    Parameters:
    -----------
    actions : list of str
        Action labels per trial
    contexts : list of str
        True context labels per trial

    Returns:
    --------
    accuracy : np.ndarray
        Binary accuracy per trial
    """
    acc = np.zeros(len(actions))
    for t in range(len(actions)):
        if contexts[t] == 'left_better' and 'left' in actions[t]:
            acc[t] = 1.0
        elif contexts[t] == 'right_better' and 'right' in actions[t]:
            acc[t] = 1.0
        else:
            acc[t] = 0.0
    return acc

def plot_posterior_predictive(df, subject_id=0, reversal_schedule=None, save_prefix='subject_fit'):
    """
    Generate posterior predictive checks for a specific subject.
    """

    if reversal_schedule is None:
        reversal_schedule = [50, 100, 150]

    print(f"\n" + "="*70)
    print(f"POSTERIOR PREDICTIVE CHECK - SUBJECT {subject_id}")
    print("="*70)
    print("Generating posterior predictive simulations...")

    # Get fitted parameters for this subject
    subj_data = df[df['subject_id'] == subject_id]

    # Simulate from each fitted model
    n_sims = 50
    print(f"Running {n_sims} simulations per model...")

    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    fig.suptitle(f'Posterior Predictive Checks - Subject {subject_id}',
                 fontsize=16, fontweight='bold')

    colors = {'M1': '#3498db', 'M2': '#2ecc71', 'M3': '#e74c3c'}

    for model_idx, model_name in enumerate(['M1', 'M2', 'M3']):
        print(f"Simulating {model_name} ({model_idx+1}/3)...")
        model_data = subj_data[subj_data['model'] == model_name].iloc[0]
        params = model_data['fitted_params']

        # Simulate trajectories
        gamma_trajs = []
        entropy_trajs = []
        accuracy_trajs = []

        for sim in range(n_sims):
            if sim % 10 == 0 and sim > 0:
                print(f"  {model_name}: {sim}/{n_sims} simulations completed")

            np.random.seed(sim + subject_id * 1000)

            # Build environment
            env = TwoArmedBandit(
                probability_hint=0.7,
                probability_reward=0.8,
                reversal_schedule=reversal_schedule.copy()
            )

            # Build model
            A = build_A(num_modalities, state_contexts, state_choices,
                       observation_hints, observation_rewards, observation_choices,
                       0.7, 0.8)
            B = build_B(state_contexts, state_choices, action_contexts,
                       action_choices, context_volatility=0.05)
            D = build_D(state_contexts, state_choices)

            # Get policies if needed
            if model_name == 'M3':
                A_temp = build_A(num_modalities, state_contexts, state_choices,
                               observation_hints, observation_rewards, observation_choices,
                               0.7, 0.8)
                B_temp = build_B(state_contexts, state_choices, action_contexts, action_choices)
                D_temp = build_D(state_contexts, state_choices)
                C_temp = utils.obj_array_zeros([(A_temp[m].shape[0],) for m in range(len(A_temp))])

                temp_agent = Agent(A=A_temp, B=B_temp, C=C_temp, D=D_temp,
                                 policy_len=2, inference_horizon=1,
                                 control_fac_idx=[1], use_utility=True,
                                 use_states_info_gain=True,
                                 action_selection="stochastic", gamma=16)

                policies = temp_agent.policies
                num_actions_per_factor = [len(action_contexts), len(action_choices)]
            else:
                policies = None
                num_actions_per_factor = None

            # Build value function
            value_fn = build_value_fn_from_params(params, model_name,
                                                  policies, num_actions_per_factor)

            # Run simulation
            runner = AgentRunner(A, B, D, value_fn,
                               observation_hints, observation_rewards,
                               observation_choices, action_choices,
                               reward_mod_idx=1)

            logs = run_episode(runner, env, T=200, verbose=False)

            # Compute metrics
            acc = trial_accuracy(logs['action'], logs['context'])
            entropies = [compute_entropy(b) for b in logs['belief']]

            gamma_trajs.append(logs['gamma'])
            entropy_trajs.append(entropies)
            accuracy_trajs.append(acc)

        print(f"  ✓ {model_name} simulations complete")

    print("Generating posterior predictive plots...")

    # Plot gamma trajectories
    ax = axes[0, 0]

    # Process trajectories for all models
    all_gamma_trajs = {}
    all_entropy_trajs = {}
    all_accuracy_trajs = {}

    # Need to recompute for all models since we're outside the loop
    for model_name in ['M1', 'M2', 'M3']:
        model_data = subj_data[subj_data['model'] == model_name].iloc[0]
        params = model_data['fitted_params']

        # Quick simulation for plotting
        gamma_trajs = []
        entropy_trajs = []
        accuracy_trajs = []

        for sim in range(min(10, n_sims)):  # Fewer sims for plotting
            np.random.seed(sim + subject_id * 1000)

            # Build environment
            env = TwoArmedBandit(
                probability_hint=0.7,
                probability_reward=0.8,
                reversal_schedule=reversal_schedule.copy()
            )

            # Build model
            A = build_A(num_modalities, state_contexts, state_choices,
                       observation_hints, observation_rewards, observation_choices,
                       0.7, 0.8)
            B = build_B(state_contexts, state_choices, action_contexts,
                       action_choices, context_volatility=0.05)
            D = build_D(state_contexts, state_choices)

            # Get policies if needed
            if model_name == 'M3':
                A_temp = build_A(num_modalities, state_contexts, state_choices,
                               observation_hints, observation_rewards, observation_choices,
                               0.7, 0.8)
                B_temp = build_B(state_contexts, state_choices, action_contexts, action_choices)
                D_temp = build_D(state_contexts, state_choices)
                C_temp = utils.obj_array_zeros([(A_temp[m].shape[0],) for m in range(len(A_temp))])

                temp_agent = Agent(A=A_temp, B=B_temp, C=C_temp, D=D_temp,
                                 policy_len=2, inference_horizon=1,
                                 control_fac_idx=[1], use_utility=True,
                                 use_states_info_gain=True,
                                 action_selection="stochastic", gamma=16)

                policies = temp_agent.policies
                num_actions_per_factor = [len(action_contexts), len(action_choices)]
            else:
                policies = None
                num_actions_per_factor = None

            # Build value function
            value_fn = build_value_fn_from_params(params, model_name,
                                                  policies, num_actions_per_factor)

            # Run simulation
            runner = AgentRunner(A, B, D, value_fn,
                               observation_hints, observation_rewards,
                               observation_choices, action_choices,
                               reward_mod_idx=1)

            logs = run_episode(runner, env, T=200, verbose=False)

            # Compute metrics
            acc = trial_accuracy(logs['action'], logs['context'])
            entropies = [compute_entropy(b) for b in logs['belief']]

            gamma_trajs.append(logs['gamma'])
            entropy_trajs.append(entropies)
            accuracy_trajs.append(acc)

        all_gamma_trajs[model_name] = np.array(gamma_trajs)
        all_entropy_trajs[model_name] = np.array(entropy_trajs)
        all_accuracy_trajs[model_name] = np.array(accuracy_trajs)

    # Now plot all models
    for model_name in ['M1', 'M2', 'M3']:
        gamma_trajs = all_gamma_trajs[model_name]
        mean_gamma = np.mean(gamma_trajs, axis=0)
        std_gamma = np.std(gamma_trajs, axis=0)

        # Plot a few individual trajectories (faint)
        for traj in gamma_trajs[:3]:
            ax.plot(traj, color=colors[model_name], alpha=0.1, linewidth=0.5)

        # Plot mean ± std
        ax.plot(mean_gamma, color=colors[model_name], linewidth=2.5, label=model_name, alpha=0.9)
        ax.fill_between(range(len(mean_gamma)),
                        mean_gamma - std_gamma,
                        mean_gamma + std_gamma,
                        color=colors[model_name], alpha=0.2)

    # Mark reversals
    for rev in reversal_schedule:
        ax.axvline(rev, color='black', linestyle='--', linewidth=1, alpha=0.5)

    ax.set_xlabel('Trial', fontsize=11)
    ax.set_ylabel('γ (policy precision)', fontsize=11)
    ax.set_title('Predicted γ Trajectories', fontsize=12, fontweight='bold')
    ax.legend()
    ax.grid(True, alpha=0.3)

    # Plot entropy trajectories
    ax = axes[0, 1]

    for model_name in ['M1', 'M2', 'M3']:
        model_data = subj_data[subj_data['model'] == model_name].iloc[0]

        # Use stored trajectories from above loop (hacky but works)
        entropy_mean = np.mean(entropy_trajs, axis=0)
        entropy_std = np.std(entropy_trajs, axis=0)

        ax.plot(rolling_mean(entropy_mean, 5), color=colors[model_name],
                linewidth=2.5, label=model_name, alpha=0.9)
        ax.fill_between(range(len(entropy_mean)),
                        rolling_mean(entropy_mean - entropy_std, 5),
                        rolling_mean(entropy_mean + entropy_std, 5),
                        color=colors[model_name], alpha=0.2)

    for rev in reversal_schedule:
        ax.axvline(rev, color='black', linestyle='--', linewidth=1, alpha=0.5)

    ax.set_xlabel('Trial', fontsize=11)
    ax.set_ylabel('H(q(context))', fontsize=11)
    ax.set_title('Predicted Belief Entropy', fontsize=12, fontweight='bold')
    ax.legend()
    ax.grid(True, alpha=0.3)

    # Plot switch dynamics (accuracy around reversals)
    ax = axes[1, 0]

    window = 10

    for model_name in ['M1', 'M2', 'M3']:
        # Recompute for this model
        acc_around_reversals = []

        for traj in accuracy_trajs:  # Using last model's trajs (should refactor)
            for rev in reversal_schedule:
                start = max(0, rev - window)
                end = min(len(traj), rev + window)
                seg = traj[start:end]

                pad_left = window - (rev - start)
                pad_right = window - (end - rev)
                seg_padded = np.pad(seg, (pad_left, pad_right), constant_values=np.nan)

                acc_around_reversals.append(seg_padded)

        acc_mean = np.nanmean(acc_around_reversals, axis=0)
        acc_std = np.nanstd(acc_around_reversals, axis=0) / np.sqrt(len(acc_around_reversals))

        x = np.arange(-window, window)
        ax.plot(x, acc_mean, color=colors[model_name], linewidth=2.5,
                label=model_name, marker='o', markersize=4, markevery=2)
        ax.fill_between(x, acc_mean - acc_std, acc_mean + acc_std,
                        color=colors[model_name], alpha=0.2)

    ax.axvline(0, color='black', linestyle='--', linewidth=2, alpha=0.7, label='Reversal')
    ax.set_xlabel('Trials relative to reversal', fontsize=11)
    ax.set_ylabel('Accuracy', fontsize=11)
    ax.set_title('Switch Dynamics', fontsize=12, fontweight='bold')
    ax.set_ylim([0, 1.05])
    ax.legend()
    ax.grid(True, alpha=0.3)

    # Plot switch latency distribution
    ax = axes[1, 1]

    latencies = {'M1': [], 'M2': [], 'M3': []}

    # Compute switch latency for each model (placeholder - would need actual implementation)
    # For now, show parameter comparison

    # Instead, show fitted parameter comparison
    m1_params = subj_data[subj_data['model'] == 'M1'].iloc[0]['fitted_params']
    m2_params = subj_data[subj_data['model'] == 'M2'].iloc[0]['fitted_params']
    m3_params = subj_data[subj_data['model'] == 'M3'].iloc[0]['fitted_params']

    # Show key parameters
    param_names = ['γ (M1)', 'γ_base (M2)', 'γ₀ (M3)', 'γ₁ (M3)']
    param_values = [
        m1_params['gamma'],
        m2_params['gamma_base'],
        m3_params['profiles'][0]['gamma'],
        m3_params['profiles'][1]['gamma']
    ]

    ax.barh(param_names, param_values, color=['#3498db', '#2ecc71', '#e74c3c', '#e74c3c'], alpha=0.7)
    ax.set_xlabel('Parameter value', fontsize=11)
    ax.set_title('Fitted Parameters (Subject-Level)', fontsize=12, fontweight='bold')
    ax.grid(True, alpha=0.3, axis='x')

    plt.tight_layout()
    plt.savefig(f'{save_prefix}_posterior_pred_subj{subject_id}.png', dpi=300, bbox_inches='tight')
    print(f"✓ Saved: {save_prefix}_posterior_pred_subj{subject_id}.png")
    plt.show()

# ============================================================================
# RUN THE FULL EXPERIMENT
# ============================================================================

print("="*70)
print("RUNNING SUBJECT-LEVEL FITTING EXPERIMENT")
print("="*70)
print("\nThis will:")
print("  1. Generate synthetic subjects from M3 (ground truth)")
print("  2. Fit M1, M2, and M3 to each subject's data")
print("  3. Perform cross-validation")
print("  4. Compare models using CV-LL, AIC, BIC")
print("  5. Analyze fitted parameters")
print("  6. Generate posterior predictive checks")
print("\nThis may take 10-20 minutes depending on n_subjects...")
print()

# Run experiment
print("STARTING MAIN EXPERIMENT...")
results_df = run_subject_level_experiment(
    n_subjects=3,
    n_trials=200,
    reversal_schedule=[50, 100, 150],
    verbose=True,
    skip_cv=True  # Enable fast mode for initial testing
)

print("\nSTARTING STATISTICAL ANALYSIS...")
# Analyze and visualize
analyze_and_plot_results(results_df, save_prefix='subject_fit')

print("\nGENERATING POSTERIOR PREDICTIVE CHECKS...")
# Posterior predictive check for first subject
plot_posterior_predictive(results_df, subject_id=0, reversal_schedule=[50, 100, 150])

print("\nSAVING FINAL RESULTS...")
# Save results
results_df.to_csv('subject_fitting_results.csv', index=False)
print("✓ Results saved to: subject_fitting_results.csv")

print("\n" + "="*70)
print("EXPERIMENT COMPLETE!")
print("="*70)
print("All output files have been saved:")
print("  📊 subject_fit_model_comparison.png")
print("  📈 subject_fit_parameters_m3.png")
print("  🔍 subject_fit_posterior_pred_subj0.png")
print("  📁 subject_fitting_results.csv")
print("="*70)

