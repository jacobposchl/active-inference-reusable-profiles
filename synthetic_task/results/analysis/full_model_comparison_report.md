# Full Model Comparison Report

Date: 2025-11-12

This report documents the model-comparison experiments you ran with `src/experiments/model_comparison.py` and the aggregated analysis produced by `src/experiments/aggregate_results.py`.
It integrates the numeric summaries and paired statistical tests computed from the per-run CSVs (`results/csv/*.csv`) and provides a detailed interpretation and recommended next steps.

## Contents
- Executive summary
- Experimental setup and generative task
- Model descriptions (M1, M2, M3)
- Metrics and how they were computed
- Results (tables, confidence intervals, paired tests)
- Mechanistic interpretation
- Limitations and recommended next steps

---

## Executive summary

- Three models (M1, M2, M3) were compared on a two-armed bandit with periodic reversals.
- Experiments tested reversal intervals of 40, 80 and 160 trials (20 independent runs per model, 800 trials per run). Per-run metrics were saved and aggregated.
- Across all tested intervals M3 (profile-based, state-conditional mixing) decisively outperformed M1 and M2 in predictive fit (log-likelihood) and model selection criteria (AIC, BIC). Differences in log-likelihood were very large (hundreds of log-likelihood units) and paired tests show p ≪ 0.001 for LL comparisons.
- Behavioral performance (accuracy, total reward) generally improved as reversals became less frequent. Accuracy differences between models were small in absolute terms but some were statistically significant (paired tests), while total reward differences were typically not significant.

---

## Experimental setup and generative task

- Environment: Two-armed bandit with
  - Context latent: `['left_better', 'right_better']`
  - Choice states: `['start', 'hint', 'left', 'right']`
  - Actions observed: `['act_start', 'act_hint', 'act_left', 'act_right']`
  - Observations: hints (reliable, p_hint = 0.99), rewards (p_reward = 0.85), choice observations
- Reversal schedules tested:
  - interval = 40 (reversals at 40, 80, 120, ...)
  - interval = 80
  - interval = 160
- Runs: 20 independent runs per model per interval
- Trials per run: 800
- Seed: 42 with run offset (seed + run_idx) so results are reproducible and roughly paired across models

All agents used the same generative model (A, B, D) but different value functions (M1/M2/M3) as described next.

---

## Candidate models (brief, precise)

- M1 — Static preferences and static precision
  - Fixed outcome preferences C (softmax of logits) and fixed policy precision γ (scalar).
  - Low complexity: k = 3 free parameters (as used in AIC/BIC calculations).

- M2 — Static preferences with entropy-coupled precision
  - Fixed outcome preferences, but γ adapts via an entropy-coupled schedule:
    gamma = gamma_base / (1 + k * H(q_context)).
  - Slightly more flexible: k = 4 parameters.

- M3 — Profile-based model (state-conditional mixing)
  - Two profiles (one per context) each with outcome preference logits (phi), action logits (xi, mapped to policy priors), and gamma.
  - Profiles are mixed according to context beliefs via an assignment matrix Z.
  - Higher complexity: k = 14 free parameters (careful counting of free elements in phi, xi, gammas and Z constraints).

---

## Metrics — definitions and computation

- Accuracy: trial-by-trial correctness computed by `trial_accuracy` (1 = chose the better arm given true context, 0 otherwise). Reported: mean accuracy per run, then average ± std across the 20 runs.
- Total reward: numeric encoding of reward observations per trial (+1 reward, -1 loss, 0 null) summed across trials.
- Adaptation time: for each reversal, the number of trials until post-reversal rolling accuracy crosses 0.7 (windowed check). Reported as mean across reversals and runs.
- Gamma (γ): mean policy precision across trials per run, then summarized across runs.
- Log-Likelihood (LL): per-trial log-probability of the chosen action under the model's policy posterior q_pi (computed in AgentRunnerWithLL). Run-level LL = sum of per-trial LL across T trials. Aggregated across runs: mean ± std; 95% CI computed by bootstrap (as implemented in the scripts).
- AIC/BIC: computed per run using each run's LL and model parameter count k:
  - AIC_i = 2*k - 2*LL_i
  - BIC_i = k * log(n) - 2*LL_i  (n = number of trials = 800)
  Then reported mean ± std across runs.

Statistical comparisons were performed by aligning runs by `run_idx` and computing paired tests (M3 vs M1 and M3 vs M2) for selected metrics: mean_accuracy, log_likelihood, total_reward. Tests reported: paired t-test, Wilcoxon signed-rank, paired Cohen's d (effect size), and bootstrap 95% CI for mean difference.

---

## Key numeric results (aggregated)

Below are compact per-interval summary tables for the key metrics (means ± std, and LL 95% CI when available). Values are taken from `results/summary/model_comparison_summary.csv` generated by the aggregation script.

### Interval = 40

| Model | Accuracy (mean ± std) | Total Reward (mean ± std) | LL (mean ± std) | LL 95% CI | AIC (mean ± std) | BIC (mean ± std) |
|---:|---:|---:|---:|---:|---:|---:|
| M1 | 0.91219 ± 0.01150 | 462.4 ± 22.55 | -353.82 ± 3.08 | [-355.27, -352.38] | 713.65 ± 6.16 | 727.70 ± 6.16 |
| M2 | 0.91156 ± 0.01108 | 461.8 ± 20.98 | -426.73 ± 5.55 | [-429.32, -424.13] | 861.45 ± 11.09 | 880.19 ± 11.09 |
| M3 | 0.91188 ± 0.01128 | 462.1 ± 22.01 | -39.13 ± 2.86 | [-40.47, -37.79] | 106.26 ± 5.72 | 171.84 ± 5.72 |

Observations: M3's LL is dramatically better (much less negative) than M1/M2 — by several hundred units — producing vastly lower AIC/BIC despite higher complexity.

### Interval = 80

| Model | Accuracy (mean ± std) | Total Reward (mean ± std) | LL (mean ± std) | LL 95% CI | AIC (mean ± std) | BIC (mean ± std) |
|---:|---:|---:|---:|---:|---:|---:|
| M1 | 0.94356 ± 0.01028 | 494.8 ± 24.03 | -349.24 ± 3.26 | [-350.77, -347.72] | 704.49 ± 6.52 | 718.54 ± 6.52 |
| M2 | 0.94250 ± 0.01011 | 494.3 ± 22.64 | -418.59 ± 5.93 | [-421.36, -415.81] | 845.17 ± 11.85 | 863.91 ± 11.85 |
| M3 | 0.94294 ± 0.01007 | 494.6 ± 23.66 | -35.35 ± 2.78 | [-36.65, -34.05] | 98.70 ± 5.55 | 164.28 ± 5.55 |

### Interval = 160

| Model | Accuracy (mean ± std) | Total Reward (mean ± std) | LL (mean ± std) | LL 95% CI | AIC (mean ± std) | BIC (mean ± std) |
|---:|---:|---:|---:|---:|---:|---:|
| M1 | 0.95694 ± 0.01140 | 510.8 ± 24.31 | -347.11 ± 3.26 | [-348.64, -345.58] | 700.22 ± 6.53 | 714.28 ± 6.53 |
| M2 | 0.95575 ± 0.01090 | 509.7 ± 23.09 | -414.66 ± 5.88 | [-417.41, -411.90] | 837.31 ± 11.76 | 856.05 ± 11.76 |
| M3 | 0.95637 ± 0.01093 | 510.7 ± 23.68 | -33.85 ± 2.88 | [-35.19, -32.50] | 95.69 ± 5.76 | 161.27 ± 5.76 |

Notes on trends:

- Accuracy and cumulative reward increase as reversals become less frequent (40 → 80 → 160), as expected.
- M3 consistently yields the best fit (LL) and best AIC/BIC across all intervals.
- The LL gaps are very large (M3 LL ~ -33..-39 vs M1 LL ~ -347..-354 vs M2 LL ~ -417..-427). These correspond to per-run differences on the order of hundreds of log-likelihood units across 800 trials.

---

## Paired statistical tests (M3 vs M1 and M3 vs M2)

The aggregation script computed paired paired t-tests, Wilcoxon signed-rank tests, Cohen's d for paired samples, and bootstrap 95% CIs for the mean differences. Results are saved in `results/summary/model_comparison_stats.csv`. Selected highlights:

- Log-Likelihood: for every tested interval, the paired t-test comparing M3 vs M1 (and M3 vs M2) yields p ≪ 0.001 (machine precision), reflecting the enormous LL advantage of M3. The mean LL differences are ~300+ in favor of M3 (M3 less negative), and bootstrap CIs for mean difference exclude zero comfortably. Example (interval 160, M3 vs M1): t ≈ 1006.5, p ≈ 2.2e-46, mean_diff ≈ 313.27 (bootstrap CI ≈ [312.65, 313.89]).

- Accuracy: differences are small in absolute terms (fractions of a percent) but can be statistically significant at conventional alpha=0.05 depending on interval and comparison. Example (interval 160, M3 vs M1): paired t-stat = -2.44, p = 0.0248, Wilcoxon p = 0.0412, Cohen's d ≈ -0.545 — a moderate effect in the negative direction (here negative means M3 slightly *lower* mean_accuracy than M1 for that interval), though absolute magnitude is tiny (~6e-4).

- Total reward: generally not significant in paired tests across intervals (p-values typically > 0.1), indicating that while M3 has a huge likelihood advantage, cumulative reward differences across runs are smaller and more variable.

In short: LL differences are massively significant (statistically and practically). Accuracy differences are small and sometimes reach statistical significance because the runs are paired and variance is small; however their practical magnitude (fractional percent) is limited.

---

## Mechanistic interpretation

Why does M3 win so strongly?

- Structural match: M3 encodes state-conditional profiles (phi/xi/gamma per profile) and mixes them via beliefs about context. The environment has a simple latent context (left_better vs right_better) and reliable hints (p_hint=0.99). A model that can condition its action priors and precision on context will match observed behaviour of an agent that exploits state-conditional strategies.
- Determinism/precision: profile gammas in M3 are higher (2.5) than the average gammas in M1/M2; combined with profile-specific priors, this leads to action posteriors that put high mass on the observed actions, generating high per-trial LL.
- Model flexibility but parsimony: although M3 is more complex (k=14), the LL improvement magnitude overwhelms the complexity penalty in AIC/BIC.

Mechanistic plots (short-run analysis):

- The mechanistic script (`src/experiments/mechanistic_analysis.py`) generates plots aligned to reversal points (90-trial episodes with reversals at 30 and 60). These visualizations show how accuracy and gamma evolve around reversals and how M3’s profile weights switch when beliefs change — providing causal insight into *how* M3 uses state-conditional priors to produce better predictions.

---

## Limitations and caveats

- LL scale: The huge LL differences indicate that M3 assigns very high probability mass to observed actions; ensure the LL calculation matches your inferential goals (it uses the posterior over policies and sums posterior mass for policies that prescribe the chosen first action). This is reasonable for short-horizon policies but be aware of interpretation.
- AIC/BIC and repeated simulations: AIC/BIC are traditionally used for single-dataset model selection after fitting; here we compute AIC/BIC per run (using simulated choices) and summarize across runs. This is a reasonable pragmatic approach for simulated comparisons, but for empirical fitting you would fit parameters to data and then compute information criteria.
- Practical significance vs statistical significance: small numeric differences in accuracy can be statistically significant with paired tests when run variance is low and sample size is 20. Interpret effect sizes (Cohen's d and raw mean differences) alongside p-values.
- Missing 320-interval CSV: the aggregation used the CSVs present under `results/csv/`; if a 320-interval CSV is present elsewhere or was generated only to console, re-run the experiment or place the CSV in `results/csv/` so aggregation includes it.

---

## Recommended next steps

1. Persisting outputs and reproducibility
   - Keep the per-run CSVs under `results/csv/` (already done). Archive them alongside code and seed lists for full reproducibility.

2. Additional statistics and visualization
   - Add plots of per-run paired differences (e.g., violin + paired lines) for LL and accuracy to visualize distributions and paired structure.
   - Report median differences and robust CIs in addition to means.

3. Robustness checks
   - Vary environment parameters (hint reliability, reward probability, context volatility) to test robustness of M3 advantage.
   - Run parameter recovery: generate synthetic datasets under each model and attempt to recover the generating model via fitting.

4. Cross-validation / predictive checks
   - Fit model parameters on a subset of runs and evaluate predictive log-likelihood on held-out runs or hold-out trials within runs.

5. Mechanistic inspection
   - Use the short-run mechanistic plots to produce a small figure panel for a manuscript: (a) accuracy around reversals (pooled), (b) gamma dynamics around reversals, (c) M3 profile weights around reversals, (d) example single-run trajectories with annotated reversals.

---

## Files and outputs referenced in this report

- Per-run CSVs: `results/csv/model_comparison_interval_*.csv`
- Aggregated summary CSV: `results/summary/model_comparison_summary.csv`
- Aggregated summary Markdown: `results/summary/model_comparison_summary.md`
- Paired-stats CSV: `results/summary/model_comparison_stats.csv`
- Figures from main experiment: `results/figures/model_comparison_interval_<interval>.png`
- Mechanistic figures: `results/figures/mechanistic_accuracy_rev_30_60.png`, `mechanistic_gamma_rev_30_60.png`, `mechanistic_m3_profiles_rev_30_60.png`

---

If you want, I can now:
- embed the full aggregated tables directly into this report (with CSV append) so the Markdown is self-contained, or
- generate the paired-difference plots and add them to `results/figures/` and append them to this report,
- or implement parameter-recovery and cross-validation scripts and summarize their results here.

Which would you like next?
